url,title,content,category,keyword,category_explanation,keyword_explanation
https://speakerdeck.com/gaelvaroquaux/open-source-software-how-to-live-long-and-go-far,Open source software: how to live long and go far - Speaker Deck,"Open source software: how to live long and go far Open source software: how to live long and go far An opinionated guide to building open-source software toolswith a focus on Python and science A talk that I gave when I was stepping down as a lead for the nilearn software, passing the baton to new maintainers. My goal is to summarize what I have learned across the years as a maintainer of open source/  Gael Varoquaux More Decks by Gael Varoquaux Other Decks in Programming Featured Transcript Open source software: how to live long and go far Open source software: how to live long and go far A project vision is crucial Outline a clear vision & Building with a community Community-driven "" Open source G Varoquaux Building with a community Community-driven "" Open source A community Building with a community Community-driven "" Open source A community Building with a community Community-driven "" Open source A community Limiting complexity G Varoquaux 5 Limiting complexity: being inclusive means keeping it simple Every required Limiting complexity: understanding is harder than building Debugging is twice Limiting complexity: a super-linear cost Features are costly [An Experiment Technical debt: today’s asset may be tomorrow’s liability Choose widely Technical debt: today’s asset may be tomorrow’s liability Choose widely Limiting drag Compiled code increases severely burden Installation is where Limiting drag Navigating the trade-oﬀs Compiled code increases severely burden Design: innovation and product design G Varoquaux 11 Design: innovation and product design G Varoquaux 11 Design: innovation and product design Use technical sophistication to ﬁnd Design: ergonomics Separating control of temperature and ﬂow rate Rethinking Design: Some API design principles for Python tools Consistency, consistency, Design: Some API design principles for Python tools Consistency, consistency, Design: Some API design principles for Python tools Consistency, consistency, Design: Some API design principles for Python tools Consistency, consistency, Design: Some API design principles for Python tools Consistency, consistency, Design: Some API design principles for Python tools Consistency, consistency, Design: Some API design principles for Python tools Consistency, consistency, Design: Some API design principles for Python tools Consistency, consistency, Design: Some API design principles for Python tools Consistency, consistency, Design: Some API design principles for Python tools Consistency, consistency, Design: Some API design principles for Python tools Consistency, consistency, Iterations: Short cycles, limited ambitions G Varoquaux 24 Iterations: Short cycles, limited ambitions Keep coming back to your Iterations: Limited resources Limited resources are good Need success in Quality: a priority G Varoquaux 26 Quality: a priority Quality will give you users Bugs give Quality: a priority Quality will give you users Bugs give Quality: everywhere Great documentation Simplify, but don’t dumb down Focus Quality: everywhere Great documentation Simplify, but don’t dumb down Focus Quality: Process Code review Everything is discussed ñ foster knowledge Users ﬁrst: Enabling Usability is key Users are not stupid, Users ﬁrst: reducing their cognitive load Design: Jonathan Ive, an @GaelVaroquaux Open source software: how to live long and go @GaelVaroquaux Open source software: how to live long and go SpeakerDeck Top Categories Use Cases Resources Features Copyright © 2025 Speaker Deck, LLC. All slide content and descriptions are owned by their creators.",general tools,"open-source software, Python, community","The article provides a guide to building open-source software, focusing on Python and science.",Open-source software is the central topic of the article.; Python is mentioned as the primary language for the software; Community is emphasized throughout the text
https://medium.com/@seanjtaylor/a-personal-retrospective-on-prophet-f223c2378985,A Personal Retrospective on Prophet | by Sean J. Taylor | Medium,"Sign up Sign in Sign up Sign in A Personal Retrospective on Prophet -- 1 Listen Share February 2017 was a pretty exciting time for me.Ben Lethamand I were on the cusp ofopen sourcing Prophet, now a popular forecasting library but at the time just a library for internal use at Facebook. We had developed the method for a specific forecasting use-case, and then gotten a few additional wins helping internal teams improve the accuracy of their growth forecasts. The Prophet open source launch was so successful I was taken by surprise, but with hindsight we had a lot of things going for us: We were invited to give manytalks about Prophetand a lot of people now associate me quite closely with the project. Being a co-creator of a popular open source project has had a number of unseen consequences, and I think it’s instructive to share this more personal part of the journey and some lessons I’ve learned. Unearned credibility Prophet isan approach to forecastingborn out of pragmatism — at the time we had found few off-the-shelf methods that were easy to use and accounted for the complexities we faced. It just wasn’t a rigorous research project like a forecasting researcher might complete. We validated the method on a few use cases at Facebook and then deemed it worth sharing. Some people now include me in a group of expert time-series forecasters, when a more accurate portrayal would be that I spent about a year working on a specific forecasting problem, had a great collaborator in Ben, and was pretty good at writing usable software. I think anyone who’s an actual forecasting expert could justifiably feel we got a little more credit that we deserved for the project. I regret not engaging with the forecasting community earlier in the project and trying to understand how the method fit within existing approaches. A more thorough research project might have helped us create a more broadly useful technique, rather than something tailored to the use cases we were tackling. Contribution guilt After open sourcing Prophet I couldn’t keep up with the community in helping to maintain the project. Part of it was prioritization — I am usually working on a variety of things and I viewed the project as “finished” in the sense that I had already made the improvements I wanted for the teams at Facebook. Ben took on a lot of maintenance work, andother folksstepped in and helped do the dirty work to make sure people could continue to painlessly install and use the software. I’ll never be able to fully transfer the credit I get over to them, but I wish I could. I’m always going to feel like I abandoned our users by not participating more fully in the ongoing improvements and maintenance. The tool creator’s dilemma I did not face some difficult choice about whether to share Prophet, it felt obviously useful and relatively low cost to open it up. But I now feel some misgivings about it on a recurring basis — it’s hard for me to conclude it was unambiguously positive to share it publicly. Many folks would have been worse off if Prophet were not open sourced (I’ve heard many success stories!), and the competition that it helped foster in the forecasting software space has been very beneficial for practitioners. But there are many plausible negative effects as well, as people mis-apply Prophet to problems and overly trust the resulting forecasts. The central problem is that the method isn’t as great or general as some people believe it to be. I sum this up here: I do actually feel a bit of shame about the project at this point — of course I wish it worked better! Many people go out andfind obvious problemsin Prophet forecasts, or they build better methods and demonstrate that it is a worse method. A large number of the citations on theForecasting at scalepaper are showing it underperforming other approaches. I see no flaws with these studies, Prophet is not a reasonable model in many settings. I’m glad people are motivated to create better approaches and to do the evaluations needed to show they are superior. I wish I could definitively communicate that I never intended for Prophet to be the best and that I believe there are better options in many cases. One reason I haven’t continued to work on forecasting is because there arebetter researchersto work on it andbetterapproachesavailableto build on. But I struggle with how to “unspread” an already widely proliferated tool — I guess this post may help a bit, but ultimately it’s challenging to communicate the nuanced view that people should consider a variety of methods besides Prophet and carefully choose among them based on their requirements. Sharp edges A running joke on Twitter is thatProphet was the causeof Zillow’s 2021 stock price collapse (some folksdidn’t really get the joke). This bit is very funny to me because it takes the “overly trusting your tools” pathology to the logical extreme:what if you bet billions of dollars on the accuracy of models you didn’t understand and didn’t eval",ai and legal systems,"forecasting library, Prophet, time-series","The article discusses an AI tool (Prophet) used for forecasting, which can be applied in various fields including legal systems.",Forecasting library is the main topic of the article.; Prophet is the specific AI tool discussed.; Time-series indicates the type of data this tool works with.
https://github.com/jackboyla/GLiREL,GitHub - jackboyla/GLiREL: Generalist and Lightweight Model for Relation Extraction (Extract any relationship types from text),"Navigation Menu Search code, repositories, users, issues, pull requests... Provide feedback We read every piece of feedback, and take your input very seriously. Saved searches Use saved searches to filter your results more quickly To see all available qualifiers, see ourdocumentation. Generalist and Lightweight Model for Relation Extraction (Extract any relationship types from text) Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. jackboyla/GLiREL Folders and files Latest commit History Repository files navigation GLiREL : Generalist and Lightweight model for Zero-Shot Relation Extraction GLiREL is a Relation Extraction model capable of classifying unseen relations given the entities within a text. This builds upon the excelent work done by Urchade Zaratiana, Nadi Tomeh, Pierre Holat, Thierry Charnois on theGLiNERlibrary which enables efficient zero-shot Named Entity Recognition.  📄 GLiREL Paper•📄 GLiNER Paper•🤗 Demo•🤗 Available models Installation Usage Once you've downloaded the GLiREL library, you can import theGLiRELclass. You can then load this model usingGLiREL.from_pretrainedand predict entities withpredict_relations. Expected Output Constrain labels In practice, we usually want to define the types of entities that can exist as a head and/or tail of a relationship. This is already implemented in GLiREL: Usage with spaCy You can also load GliREL into a regular spaCy NLP pipeline. Here's an example using an English pipeline. Expected Output Example training data NOTE that the entity indices are inclusive i.e""Binsey""is[7, 7]. This differs from spaCy where the end index is exclusive (in this case spaCy would set the indices to[7, 8]) JSONL file: License GLiRELbyJack Boylanis licensed underCC BY-NC-SA 4.0. Citation If you use code or ideas from this project, please cite: About Generalist and Lightweight Model for Relation Extraction (Extract any relationship types from text) Topics Resources Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. Stars Watchers Forks Releases9 Packages0 Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. Contributors3 Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. Languages Footer Footer navigation",ai and legal systems,"relation extraction, named entity recognition, spacy","The article discusses an AI model for relation extraction, specifically in the context of legal systems.",Relation Extraction is the central concept of the article.; Named Entity Recognition is a related technology mentioned.; Spacy is a specific tool used in this context.
https://www.latencyconf.io/sessions/pandas-should-go-extinct,Pandas Should Go Extinct | Latency Conference,"talk Pandas Should Go Extinct Eddie Atkinson Talk Description Just as fire was a great innovation over freezing to death, so too was Pandas a great innovation over Excel. But just as we no longer have bonfires in the middle of our houses for heating (we simply load a small dataframe into memory on windows), the time has come to move to better tools. Join me as I discuss the Polar opposite of Pandas for in-memory analytics, the database that quacks and why you probably don't need Databricks.  Knowledge Level Level 200: Experienced. Attendees should have an awareness of the topic but may not have practical experience. Talk Details Friday, 4 April 2025 11:30 - 11:45 Latency ConferenceThe only local conference dedicated to building secure, high performing cloud native (or even adjacent) applications. ContactTo find out more, contact us at:contact@latencyconf.io Â© 2025 Latency Conference. All Rights Reserved",general tools,"in-memory analytics, database, Databricks","The article discusses a tool (a database) that is used for in-memory analytics, and it mentions Databricks as an alternative.",In-memory analytics is the main focus of the article.; Database is the type of technology being discussed.; Databricks is mentioned as a possible alternative.
https://orbae.adastra.eco/,Orbae,"See the land conversion impacts of agriculture A common language for understanding land use change Orbae reveals what happens when agriculture expands into natural ecosystems. Working from the ground up, it calculates carbon emissions and other metrics in high resolution, then combines them to reflect any land area in the world â a farm, a country or anywhere in between. Prioritize Take action Track your progress How it works Data built from the ground up Any time land conversion is observed from space, Orbae attributes it to the right commodity. Field per field. Our supporters Supporters make Orbae possible.To help us keep the data open, join us. All crops. Anywhere in the world. Any level of traceability. All types of land conversion. This is our ambition for Orbae. Itâs a work in progress, but weâre well on our way there. Weâre prioritizing the commodities that are well-known drivers of deforestation, then expanding to others. features See your land conversion hotspots like never before Aligned with the Greenhouse Gas Protocol and Science Based Targets initiative.Fit for reporting â and more importantly, for strategic decision making. An innovation from AdAstra Sustainability, experts in environmental accounting, data science, GIS and product development. We created Orbae because we believe that good decisions start with good data. Start-up innovation project supported by Innosuisse, the Swiss Innovation Agency Why land conversion We have less than 5 years to stop converting landâ¨and start restoring it. Land conversion makes up an estimated 11% of global greenhouse gas emissions â the majority of which come from agriculture. The global deadline to stop forest loss and protect other natural ecosystems is set for 2030. better data is here Together we can stop land conversion by 2030. Stay in touch",environmental sustainability,"land conversion, agriculture emissions, Orbae","The article discusses a technology (Orbae) that helps monitor and analyze land conversion impacts, with a focus on agriculture.",Land conversion is the central concept of the article.; Agriculture emissions are discussed as one of the main drivers of deforestation.; Orbae is the tool used for monitoring and analysis.
https://cambridge-intelligence.com/mapweave/,"
		MapWeave - The Geospatial Visualization SDK that uncovers every connection	","MapWeave The geospatial visualization SDK that uncovers every connection Build groundbreaking apps that make geospatial connections clear, without the clutter. Unite your map, network, and timeline insights in a single view for the first time. Download the white paper What is MapWeave? Uniquely actionable geospatial intelligence Weave your connected events and observations into powerful geospatial visualizations. MapWeave seamlessly integrates network, timeline and geospatial views, creating rich user experiences that uncover every connection. Geospatial insights at scale MapWeave scales effortlessly, handling your biggest and most complex datasets with ease. Its dynamic adaptive rendering eliminates chart clutter, guiding users to the patterns, outliers and network structures they need to see, and making geospatial insights crystal clear at any zoom level. Skip the integration headache MapWeave works seamlessly with your trusted tech stack, thanks to its flexible open architecture. Our expanding library of basemap adapters ensures compatibility with your preferred map assets, tile servers, and data services. And with interactive demos, quick-start guides and our responsive support, you’ll be up and running in no time. Want to try it for yourself? Request a free trial Solve your toughest geospatial intelligence challenges  OSINT investigations  Communications intelligence  Maritime intelligence  Network digital twins  Cyber threat intelligence MapWeave’s features Trusted by Our customers span six continents. They range from pioneering startups to Fortune 500 companies and national governments. From the blog Maritime intelligence visualization We demonstrate graph visualization integration with Google Cloud's Spanner Graph for scalable, real-time data insights Elevate your graph and timeline data visualization: Smoother, smarter, sharper Visit the blog Company Products Resources Register for news & updates  Registered in England and Wales with Company Number 07625370 | VAT Number 113 1740 616-8 Hills Road, Cambridge, CB2 1JP. All material © Cambridge Intelligence.Read our Privacy Policy.",graph technologies,"geospatial visualization, network analysis, real-time data insights",The article discusses a geospatial visualization SDK that integrates network and timeline insights for various intelligence applications.,Geospatial visualization is the central concept of the article.; Network analysis is discussed as a key feature of the technology.; Real-time data insights are mentioned as a capability.
https://github.com/yWorks/yfiles-jupyter-graphs-for-sparql,GitHub - yWorks/yfiles-jupyter-graphs-for-sparql: The open-source adapter for working with RDF databases and SPARQL queries in Jupyter notebooks leveraging the yFiles Graphs for Jupyter plugin.,"Navigation Menu Search code, repositories, users, issues, pull requests... Provide feedback We read every piece of feedback, and take your input very seriously. Saved searches Use saved searches to filter your results more quickly To see all available qualifiers, see ourdocumentation. The open-source adapter for working with RDF databases and SPARQL queries in Jupyter notebooks leveraging the yFiles Graphs for Jupyter plugin. License Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. yWorks/yfiles-jupyter-graphs-for-sparql Folders and files Latest commit History Repository files navigation yFiles Jupyter Graphs for SPARQL  Easily visualize aSPARQLquery forRDFgraphs in a Jupyter Notebook. This packages provides an easy-to-use interface to
theyFiles Graphs for Jupyterwidget to directly visualize queries. Installation Just install it from thePython Package Index or seeREADME_DEV.mdto build it yourself. Usage Seeexamples Supported Environments The widget uses yFiles Graphs for Jupyter at its core, and therefore runs in any environment that is supported by it,
seesupported environments. Documentation The main classSparqlGraphWidgetprovides the following API: Constructor For all arguments, there is aset_[arg]andget_[arg]method. Methods Important If you want to use SELECT query types, ensure you select all three triple components—subject, predicate, and object. Otherwise, a graph cannot be constructed from the selected data.
For an example look at theGetting Startednotebook To get an overview of the data structure, you can use the following function.
The output is constrained by thelimitproperty, meaning
only a partial schema may be displayed depending on the dataset. The graph visualization can be adjusted by adding configurations to each node label or edge type with the following
functions: add_subject_configuration(predicate: Union[str, list[str]], **kwargs: Dict[str, Any]) add_object_configuration(predicate:  Union[str, list[str]], **kwargs: Dict[str, Any]) add_predicate_configuration(type:  Union[str, list[str]], **kwargs: Dict[str, Any]) add_parent_relationship_configuration(type: Union[str, list[str]], reverse: Optional[bool] = False) -> None For a detailed documentation  look at thecore widget To remove a configuration use the following functions: How configuration bindings are resolved The configuration bindings (seeadd_object_configuration, add_subject_configurationoradd_predicate_configuration) are resolved as follows: If the configuration binding is a string, the package first tries to resolve it against the item's properties
and uses the property value if available. If there is no property with the given key, the string value itself is used as
a constant binding. In case you want to create a constant string value as binding, which also happens to be a property key, use a binding
function with a constant string as return value instead. If the configuration binding is a function, the return value of the function is used as value for the respective
configuration. yFiles Graphs for Jupyter The graph visualization is provided byyFiles Graphs for Jupyter, a
versatile graph visualization widget for Jupyter Notebooks. It can import and visualize graphs from various popular Python packages
(e.g.NetworkX,PyGraphviz,igraph) or just structurednode and edge lists. And provides a rich set of visualization options to bring your data to life (see
theexample notebooks). Feature Highlights For a detailed feature guide, check out the main widgetexample notebooks Code of Conduct This project and everyone participating in it is governed by
theCode of Conduct.
By participating, you are expected to uphold this code.
Please report unacceptable behavior tocontact@yworks.com. Feedback This widget is by no means perfect.
If you find something is not working as expected
we are glad to receive an issue report from you.
Please make sure
tosearch for existing issuesfirst
and check if the issue is not an unsupported feature or known issue.
If you did not find anything related, report a new issue with necessary information.
Please also provide a clear and descriptive title and stick to the issue templates.
Seeissues. Dependencies License SeeLICENSEfile. About The open-source adapter for working with RDF databases and SPARQL queries in Jupyter notebooks leveraging the yFiles Graphs for Jupyter plugin. Topics Resources License Code of conduct Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. Stars Watchers Forks Releases2 Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. Contributors2 Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. Languages Footer Footer navigation",graph technologies,"SPARQL, RDF databases, yFiles Graphs for Jupyter","The article discusses a tool for visualizing SPARQL queries on RDF graphs within Jupyter Notebooks, using the yFiles Graphs for Jupyter plugin.",SPARQL is the query language used in this context.; RDF databases are the type of data management systems being worked with.; yFiles Graphs for Jupyter is the specific technology mentioned.
https://medium.com/eqtventures/knowledge-graph-s-and-llm-based-ontologies-have-a-very-good-shot-at-unlocking-genai-in-production-1b167533ef63,Knowledge Graph(s) and LLM-based ontologies have a very good shot at unlocking GenAI in production | by EQT Ventures | eqtventures | Medium,"Sign up Sign in Sign up Sign in eqtventures  Multi-stage VC fund, powering the next generation of founders with the support needed to build global success stories  Knowledge Graph(s) and LLM-based ontologies have a very good shot at unlocking GenAI in production -- 6 Listen Share By:Julien Hobeika,Pierrick CudonnecandAlexander Fred-Ojala In the wake of hype around ChatGPT, enterprises began dedicating budgets to generative AI (genAI) adoption. As they played around with genAI, and its potential use cases, they quickly realised that to be truly useful and effective, these models need to interact with their proprietary, “private” company data. This realization has sparked a surge of companies focused on integrating AI with enterprise data — some developing solutions to fine-tune models directly in-house, while others look to retrieval-augmented generation (RAG) as a way to incorporate company knowledge into LLM analysis and outputs. However, the deployment of “AI in production” is yet to happen. One of the main hurdles is that the quality of the data used to feed the models is lacking. Companies likeDeasieandUnstructuredare tackling this problem by focusing on metadata labelling to crack the “data foundation layer”. There is, also, another promising approach gaining attention: knowledge graphs (KGs). Graph databases are the solution to the “data foundation layer” and will unlock KGs value catch-up Graph databases have failed to capture as much value as their relational counterparts (see chart). At the scale of a single business, no graph database has achieved the same widespread adoption or impact of platforms like Snowflake or Databricks. One key reason is that relational databases power apps at scale (e.g. CRMs, ERPs etc), while graph databases do not. This is largely because no KG architecture has been able to capture and update enough data to comprehensively and accurately map a business. We believe that graph databases are the solution to enabling AI apps to move into production in the enterprise market, because they can solve the “data foundation layer” problem. The reason for this is because LLMs are a powerful way to build ontologies and semantic layers. Let’s explore why. Graph databases overcame some of the relational database limits and powered “early” AI apps, but they remain limited When Leonhard Euler (1707–1783) came up with the foundation of graph theory — while trying to prove that theKönigsberg bridge problem had no solution— he likely had no idea of the immense business potential of graph databases. In the 1970s the relational database model emerged and became the dominant paradigm for decades. Relational databases excelled at handling structured tabular data — and they still do, as exemplified by Snowflake. One of the limitations of relational databases is that they struggle with complex relationships due to their reliance on joins and normalization. Enter graph databases, which represent data as nodes (entities) and edges (relationships), mirroring graph theory. Neo4j, which recently surpassed$200min ARR, is probably the most famous example of a graph database company. Graph databases are a good solution to problems that involve real-world and contextual/relationship data in domains such as supply chains, recommendation systems, and fraud detection. They often involve complex relationships that are cumbersome to model and query in traditional databases. Graph databases were designed to represent and query these relationships natively — some great startups actually use knowledge graphs as a powerful way to build their products, such asCausalyandSpread. Problems: Graph databases have failed to scale, and gen-AI use cases are still not in production in Enterprises Scaling graph databases to handle massive amounts of data is awell-documented challenge. Moreover, they are notoriously hard to productize because ofontologies. In short, ontologies define the key entities (nodes) in a domain, their attributes, and the relationships (edges) connecting them. Until LLMs came about, ontology design was largely a manual process, often handled by consultants. This manual reliance is one of the reasons whyPalantir’s Foundry, which uses a graph-based model, became valuable but was very hard to scale as a product. LLMs and transformer-based models are particularly well-suited to define ontologies. First, they excel at capturing semantic relationships (via attention mechanism). Second, LLMs are also really good at creating vector representations (embeddings) that effectively capture the hierarchical and semantic relationships (logical) between entities. And third, they’re easy to update — a must have feature for governance as well as changing fields. As a result, LLMs are a powerful way to build ontologies, enabling knowledge graphs to be built and updated much faster. After the ontology has been “cracked”, it becomes possible to infer the graph representation of an entity — be it a business unit, process o",graph technologies,"knowledge graph, graph database, AI in production",The article discusses the role of graph databases in unlocking AI capabilities within enterprises.,Knowledge graph is a central concept as it's being used to enhance AI models.; Graph database is mentioned as the technology that can solve the 'data foundation layer' problem.; AI in production refers to the deployment of AI models within enterprises.
https://www.mongodb.com/blog/post/supercharge-ai-data-management-knowledge-graphs,Supercharge AI Data Management With Knowledge Graphs | MongoDB Blog,"Supercharge AI Data Management With Knowledge Graphs WhyHow.AI has built andopen-sourceda platform using MongoDB, enhancing how organizations leverage knowledge graphs for data management and insights. Integrated with MongoDB, this solution offers a scalable foundation with features likevector searchand aggregation to support organizations in their AI journey. Knowledge graphs address the limitations of traditionalretrieval-augmented generation(RAG) systems, which can struggle to capture intricate relationships and contextual nuances in enterprise data. By embedding rules and relationships into a graph structure, knowledge graphs enable accurate and deterministic retrieval processes. This functionality extends beyond information retrieval: knowledge graphs also serve as foundational elements for enterprise memory, helping organizations maintain structured datasets that support future model training and insights. WhyHow.AIenhances this process by offering tools designed to combinelarge language model(LLM) workflows with Python- and JSON-native graph management. Using MongoDB’s robust capabilities, these tools help combine structured and unstructured data and search capabilities, enabling efficient querying and insights across diverse datasets. MongoDB’s modular architecture seamlessly integrates vector retrieval, full-text search, and graph structures, making it an ideal platform for RAG and unlocking the full potential of contextual data. Check out ourAI Learning Hubto learn more about building AI-powered apps with MongoDB. Creating and storing knowledge graphs with WhyHow.AI and MongoDB Creating effective knowledge graphs for RAG requires a structured approach that combines workflows from LLMs, developers, and nontechnical domain experts. Simply capturing all entities and relationships from text and relying on an LLM to organize the data can lead to a messy retrieval process that lacks utility. Instead, WhyHow.AI advocates for a schema-constrained graph creation method, emphasizing the importance of developing a context-specific schema tailored to the user’s use case. This approach ensures that the knowledge graphs focus on the specific relationships that matter most to the user’s workflow. Once the knowledge graphs are created, the flexibility of MongoDB’s schema design ensures that users are not confined to rigid structures. This adaptability enables seamless expansion and evolution of knowledge graphs as data and use cases develop. Organizations can rapidly iterate during early application development without being restricted by predefined schemas. In instances where additional structure is required, MongoDB supports schema enforcement, offering a balance between flexibility and data integrity. For instance, aligning external research with patient records is crucial to delivering personalized healthcare. Knowledge graphs bridge the gap between clinical trials, best practices, and individual patient histories. New clinical guidelines can be integrated with patient records to identify which patients would benefit most from updated treatments, ensuring that the latest practices are applied to individual care plans. Optimizing knowledge graph storage and retrieval with MongoDB Harnessing the full potential of knowledge graphs requires both effective creation tools and robust systems for storage and retrieval. Here’s how WhyHow.AI and MongoDB work together to optimize the management of knowledge graphs. Storing data in MongoDB WhyHow.AI relies on MongoDB’s document-oriented structure to organize knowledge graph data into modular, purpose-specific collections, enabling efficient and flexible queries. This approach is crucial for managing complex entity relationships and ensuring accurate provenance tracking. To support this functionality, the WhyHow.AI Knowledge Graph Studio comprises several key components: Workspacesseparate documents, schemas, graphs, and associated data by project or domain, maintaining clarity and focus. Chunksare raw text segments with embeddings for similarity searches, linked to triples and documents to provide evidence and provenance. Graph collectionstores the knowledge graph along with metadata and schema associations, all organized by workspace for centralized data management. Schemasdefine the entities, relationships, and patterns within graphs, adapting dynamically to reflect new data and keep the graph relevant. Nodesrepresent entities like people, locations, or concepts, each with unique identifiers and properties, forming the graph’s foundation. Triplesdefine subject-predicate-object relationships and store embedded vectors for similarity searches, enabling reliable retrieval of relevant facts. Querieslog user queries, including triple results and metadata, providing an immutable history for analysis and optimization. To enhance data interoperability, MongoDB’s aggregation framework enables efficient linking across collections. For instance, retrieving chunks associated with ",ai and legal systems,"knowledge graph, large language model, MongoDB","The article discusses the use of AI, specifically knowledge graphs and large language models, in the context of data management for legal systems.",Knowledge graph is the central concept of the article in the context of AI and data management.; Large language model (LLM) is a specific type of AI mentioned as being integrated with knowledge graphs.; MongoDB is the technology platform used for implementing this solution.
https://enterprise-knowledge.com/the-resource-description-framework-rdf/,The Resource Description Framework (RDF) - Enterprise Knowledge,"White Paper The Resource Description Framework (RDF) Simply defined, a knowledge graph is a network of entities, their attributes, and how they’re related to one another. While these networks can be captured and stored in avariety of formats, most implementations leverage a graph based tool or database. However, within the world of graph databases, there are a variety of syntaxes or flavors that can be used to represent knowledge graphs. One of the most popular and ubiquitous is the Resource Description Framework (RDF), which provides a means to capture meaning, or semantics, in a way that is interpretable by both humans and machines. What is RDF? The Resource Description Framework (RDF) is a semantic web standard used to describe and model information for web resources or knowledge management systems. RDF consists of “triples,” or statements, with a subject, predicate, and object that resemble an English sentence. For example, take the English sentence: “Bess Schrader is employed by Enterprise Knowledge.” This sentence has: Bess Schrader and Enterprise Knowledge are two entities that are linked by the relationship “employed by.” An RDF triple representing this information would look like this:  What is the goal of using RDF? RDF is asemanticweb standard, and thus has the goal of representing meaning in a way that is interpretable by both humans and machines. As humans, we process information through a combination of our experience and logical deduction. For example, I know that “Washington, D.C.” and “Washington, District of Columbia” refer to the same concept based on my experience in the world – at some point, I learned that “D.C.” was the abbreviation for “District of Columbia.” On the other hand, if I were to encounter a breathing, living object that has no legs and moves across the ground in a slithering motion, I’d probably infer that it was a snake, even if I’d never seen this particular object before. This determination would be based on the properties I associate with snakes (animal, no legs, slithers). Unlike humans, machines have no experience on which to draw conclusions, so everything needs to be explicitly defined in order for a machine to process information this way. For example, if I want a machine to infer the type of an object based on properties (e.g. “that slithering object is a snake”), I need to define what a snake is and what properties it has. If I want a machine to reconcile that “Washington, D.C.” and “Washington, District of Columbia” are the same thing, I need to define an entity that uses both of those labels. RDF allows us to create robust semantic resources, like ontologies, taxonomies, and knowledge graphs, where the meaning behind concepts is well defined in a machine readable way. These resources can then be leveraged for any use case that requires context and meaning to connect and unify data across disparate formats and systems, such assemantic layersandauto-classification. How does RDF work? Let’s go back to our single triple representing the fact that “Bess Schrader works at Enterprise Knowledge.”  We can continue building out information about the entities in our (very small) knowledge graph by giving all of our subjects and objects types (which indicate the general category/class that an entity belongs to) and labels (which capture the language used to refer to the entity).  These types and labels are helping us define the semantics, or meaning, of each entity. By explicitly stating that “Bess Schrader” is a person and “Enterprise Knowledge” is an organization, we’re creating the building blocks for a machine to start to make inferences about these entities based on their types. Similarly, we can create a more explicit definition of our relationship and attributes, allowing machines to better understand what the “employed by” relationship means. While the above diagram represents our predicate (or relationship) as a straight line between two entities, in RDF, our predicate is itself an entity and can have its own properties (such as type, label, and description). This is often referred to as making properties “first class citizens.”  Uniform Resource Identifiers (URIs) But how do we actually make this machine readable? Diagrams in a blog are great in helping humans understand concepts, but machines need this information in a machine readable format. To make our graph machine readable, we’ll need to leverage unique identifiers. One of the key elements of any knowledge graph (RDF or otherwise) is the principle of “things, not strings.” As humans, we often use ambiguous labels (e.g. “D.C”) when referring to a concept, trusting that our audience will be able to use context to determine our meaning. However, machines often don’t have sufficient context to disambiguate strings – imagine “D.C.” has been applied as a tag to an unstructured text document. Does “D.C.” refer to the capital city of the US, the comic book publisher, “direct current,” or something else entirely? ",graph technologies,"RDF, knowledge graph, semantic web","The article discusses the Resource Description Framework (RDF), a popular and ubiquitous graph-based technology for knowledge representation.",RDF is the central topic of the article.; Knowledge graph is an essential concept in relation to RDF.; Semantic web is associated with RDF as it is a standard used for representing meaning.
https://medium.com/oracledevs/validating-graph-data-with-shacl-using-oracle-rdf-graph-adapter-for-eclipse-rdf4j-09327042f530,Validating Graph Data with SHACL Using Oracle RDF Graph Adapter for Eclipse RDF4J | by Matt Perry | Oracle Developers | Medium,"Sign up Sign in Sign up Sign in Oracle Developers  Aggregation of articles from Oracle engineers, Groundbreaker Ambassadors, Oracle ACEs, and Java Champions on all things Oracle technology. The views expressed are those of  the authors and not necessarily of Oracle.  Validating Graph Data with SHACL Using Oracle RDF Graph Adapter for Eclipse RDF4J -- Listen Share Data quality is a key part of any application. As such, when building an RDF graph application, a way to validate your RDF graph against a set of constraints to ensure its quality is critical. This article will look at a powerful new capability to validate RDF graphs stored in Oracle database using theEclipse RDF4JJava framework. The W3C has defined Shapes Constraint Language (SHACL) to specify constraints on RDF graphs. This is a major capability for the RDF ecosystem because it allows you to prevent pollution of your RDF graph with data that is invalid for your application. SHACL constraints are themselves expressed as RDF graphs using a specific vocabulary. These constraint graphs are called shapes graphs, and RDF graphs to be validated are called data graphs. The example shapes graph below specifies that any instance of the class:Personmust have at least one value for:nameand exactly one value for:birthDate. In addition, the object value of:namemust be of typexsd:string, and the object value of:birthDatemust be of typexsd:date. Eclipse RDF4Jis a popular Java framework for building RDF graph applications, and Oracle has recently (January 2025) released an update of Oracle RDF Graph Adapter for Eclipse RDF4J. This update brings the supported version of Eclipse RDF4J from 4.2.1 to 4.3.14 and includes support forSHACL validation of RDF graphs stored in Oracle Database. The remainder of this article will show, through a step-by-step example, how to validate an RDF graph stored in Oracle Autonomous Database with Java code. Step 1: Prepare an Oracle Autonomous Database Instance This step will cover how to prepare your autonomous database for Oracle RDF Graph Adapter for Eclipse RDF4J. Please seeProvision an Autonomous Database Instancefor details on how to provision an autonomous database instance. We first need to create a non-admin user for our RDF application. We will use RDFUSER for this example. On the Autonomous Database Details page in the OCI Web Console, select Database Users from the Database actions drop down menu. Ensure that your application user has some quota on the DATA tablespace. You should enable Web Access for your user, and you can optionally enable Graph and OML. Next, we need to create an RDF network in RDFUSER’s schema. Log the ADMIN user out of Database Actions and log back in as your application user. Then open a SQL Worksheet from Database Actions. Execute the following SQL statement to create a semantic network named RDF_NETWORK owned by RDFUSER. You will need to download a Wallet to use later for a JDBC connection. Click Database connection from the Autonomous Database details page. Download an Instance wallet and give a password when prompted on the subsequent page. Step 2: Prepare the Oracle RDF Graph Adapter for Eclipse RDF4J Environment This step sets up the environment for using Oracle RDF Graph Adapter for Eclipse RDF4J. As a prerequisite, ensure that you have a recent version of Java JDK 11 configured in your environment. First, download the latest Oracle RDF4J Adapter from Oracle Software Delivery Cloud. Search for “rdf4j” on the landing page. Click “REL: Oracle Adapter for Eclipse RDF4J 22.4.0.0” and then click continue. Select GENERIC for Platforms / Languages then click continue. Accept the Terms and Restrictions and then click on the file to download (V1047295–01.zip). Unzip V1047295–01.zip There should be bin, jar, and javadoc directories, and jar should contain four jar files. Create a directory for your RDF4J Java project. We are using RDF4J_SHACL_ADW in this example. Create a lib directory under RDF4J_SHACL_ADW to hold the jar files needed for the project. Copy the jars from Oracle RDF4J Adapter to the lib directory. Next, download the required jars for RDF4J and copy them to lib. Lastly, download JDBC jars for autonomous database. Download ojdbc8-full.tar.gz for Oracle Database 23ai fromhttps://www.oracle.com/database/technologies/appdev/jdbc-downloads.html.Unzip and then copy the following jars from the bundle to lib. The final contents of the lib directory should be as follows. Step 3: Using Oracle RDF Graph Adapter for Eclipse RDF4J to Validate RDF Data The Java code shown below illustrates how to use Oracle RDF4J Adapter to insert RDF data into Oracle Autonomous Database and then validate it against a SHACL shapes graph. It first adds a simple data graph to a newly created RDF Graph calledDATA_GRAPH1in the schema-private RDF network calledRDF_NETWORK. Then it uses theOracleShaclSailclass to run a transaction againstDATA_GRAPH1. RDF4J’s SHACL support evaluates SHACL constraints when transactions are com",graph technologies,"RDF graph, SHACL validation, Oracle RDF Graph Adapter",The article discusses the use of SHACL for validating RDF graphs in Oracle Database using Eclipse RDF4J.,"RDF graph is the central concept of the article, as it's the data being validated.; SHACL validation is the technique used to ensure the quality of the RDF graph.; Oracle RDF Graph Adapter is the tool mentioned for implementing SHACL validation."
https://blog.kuzudb.com/post/kuzu-wasm-rag/,Fully In-Browser Graph RAG with Kuzu-Wasm,"Fully In-Browser Graph RAG with Kuzu-Wasm We’re excited that members of our community are already building applications with theWebAssembly (Wasm)version of Kuzu,
which was only released a few weeks ago!
Early adopters to integrate Kuzu-Wasm includeAlibaba Graphscope, see:1and2, andKineviz, whose project will be launched soon. In this post, we’ll showcase the potential of Kuzu-Wasm by building a fully in-browser chatbot
that answers questions over LinkedIn data using an advanced retrieval technique: Graph
Retrieval-Augmented Generation (Graph RAG). This is achieved using Kuzu-Wasm
alongsideWebLLM, a popular in-browser LLM inference engine that can
run LLMs inside the browser. A quick introduction to WebAssembly WebAssembly (Wasm) has transformed browsers into general-purpose computing platforms.
Many fundamental software components, such as full-fledged databases, machine learning
libraries, data visualization tools, and encryption/decryption libraries, now have Wasm versions.
This enables developers to build advanced applications that run entirely in users’
browsers—without requiring backend servers. There are several benefits for building fully
in-browser applications: With this in mind, let’s now demonstrate how to develop a relatively complex AI application completely in
the browser! We’ll build afully in-browserchatbot that uses graph retrieval- augmented
generation (Graph RAG) to answer natural language questions. We demonstrate this usingKuzu-WasmandWebLLM. Architecture The high-level architecture of the application looks as follows: The term “Graph RAG” is used to refer to several techniques but in its simplest form the term
refers to a 3-step retrieval approach. The goal is to retrieve useful context from a graph DBMS (GDBMS)
to help an LLM answer natural language questions.
In our application, the additional data is information about
a user’s LinkedIn data consisting of their contacts, messages, companies the user or their contacts worked for. Yes, you can downloadyour own LinkedIn data(and you should, if
for nothing else, to see how much of your data they have!).
The schema of the graph database we use to model this data will be shown below momentarily. First, let’s go over the 3 steps of
Graph RAG: Implementation Data Ingestion The schema for our personal LinkedIn data’s graph is shown below: We ingest the data into Kuzu-Wasm in several steps using custom JavaScript code (see thesrc/utils/LinkedInDataConverter.jsfile in our Github repo): WebLLM Prompting Our code follows the exact 3 steps above. Specifically, we prompt WebLLM twice, once to create a Cypher query QCypher_{Cypher}Cypher​,
which is sent to Kuzu-Wasm.
We adapted the prompts from ourLangChain-Kuzu integration,
with a few modifications. Importantly, we make sure to include the schema information of the LinkedIn database from Kuzu in the prompt, which helps the LLM better understand
the structure and relationships (including the directionality of the relationships) in the dataset. In this example, we represented the schema as YAML instead of raw, stringified JSON in the LLM prompt.
In our anecdotal experience, for Text-to-Cypher tasks that require reasoning over the schema, we find that LLMs tend do better
with YAML syntax than they do with stringified JSON. More experiments on such Text-to-Cypher tasks will be shown in future blog posts. Observations It’s indeed impressive to see such a graph-based pipeline with LLMs being done entirely in the browser! There are, however, some caveats.
Most importantly, in the browser, resources are restricted, which limits the sizes of different components of your application.
For example, the size of the LLM you can use is limited. We tested our implementation on a MacBook Pro 2023 and a Chrome browser.
We had to choose theLlama-3.1-8B-Instruct-q4f32_1-MLCmodel (seeherefor the model card),
which is an instruction-tuned model in MLC format. Theq4f32_1format is the smallest of the Llama 3.1 models that has 8B parameters
(the largest has 450B parameters, which is of course too large to run in the browser).
For simple queries, the model performed quite well. It correctly generated Cypher queries for the LinkedIn data, such as: However, we saw that for more complex queries requiring joins, filtering, and aggregation, the model struggled to return a valid Cypher query.
It often produced incorrect or incomplete Cypher queries for questions like: “Who endorsed me the most times?“.
Token generation is also far slower than what you may be used to in state-of-the art interfaces,
such as ChatGPT. In our experiments, we observed a speed of 15-20 tokens/sec, so generating answers took on average, around 10s. Live demo We have deployed this demo so you can test it in your browser: Once the data is loaded, you can see a visualization that looks something like this: Takeaways The key takeaway from this post is that such advanced pipelines that utilize graph databases and LLMs are now possibleentirely in the browser.",graph technologies,"Graph RAG, Kuzu-Wasm, WebAssembly","The article discusses the use of WebAssembly version of Kuzu, a graph database technology, to build an in-browser chatbot.",'Graph RAG' is the central concept and technique used in the application.; Kuzu-Wasm is the specific graph database technology being used.; WebAssembly is the broader context in which Kuzu-Wasm operates.
https://blog.kuzudb.com/post/unstructured-data-to-graph-baml-kuzu/,Transforming unstructured data to a graph with BAML and Kuzu,"Transforming unstructured data to a graph with BAML and Kuzu One of the primary use cases for a property graph database like Kuzu is to explicitly model relationships between entities in your data.
However, as anyone who has worked with real-world data can attest, the vast majority of data out there is not naturally clean or structured, and may be
stored in a variety of unstructured formats like text files, PDF files, images and more.
Historically, this has created a barrier to graph database adoption due to the challenge of reliably extracting entities (nodes) and the
relationships (edges) between them from the unstructured data. It’s amply clear these days that LLMs are proving to be powerful and versatile at a large variety of tasks. In this post, we will show how to use
Kuzu in combination withBAML, a domain-specific language for generating clean, structured outputs from LLMs,
to help transform the unstructured data into a graph that can be used for downstream analysis and RAG. Using tools like BAML can help increase
trust in your LLMs’ outputs by adding some much-needed engineering rigour downstream of the generation process. We’ll also look at
some evaluation results to validate the quality and robustness of several LLMs’ outputs. The key steps in the overall workflow are summarized in the diagram below: We start with unstructured data upstream (it could be text, images, PDFs, etc.). BAML is used to prompt the LLM to generate a structured output
in JSON format, which can then be persisted as a graph in a Kuzu database.
Let’s understand this in more detail through a concrete example. Problem statement Consider this scenario: You are a developer working at a healthcare firm that has custom, proprietary data on drugs, including
their generic names, brand names, side effects and the conditions they are used to treat. Additionally,
you have a dataset of clinical notes taken by physicians that mention the side effects experienced
by patients taking these drugs. The two datasets are stored in two entirely different formats: The data on drugs is stored in a PDF file1that
contains tables with nested information,
and the clinical notes are stored in plain text, output from a custom EMR system. Let’s take a closer look
at the table inside the PDF file. There’ssomedegree of structure to the data, but it’s far from clean and
consistent, and the presence of nested structures (comma-separated items, bullet points, etc.) in each make it non-trivial
to extract clean data programmatically. Our task: Bring these two otherwise separate datasets together to construct a knowledge graph that can be queried downstream
to answer questions about patients, drugs and their side effects. Methodology We could go about transforming the PDF data into a structured form in a number of ways. The most obvious approach
would be to use a PDF parsing library likePyMuPDForpdfplumberto extract the data as text, and then prompt an LLM to extract entities and relationships from the text.
However, this approach requires a lot of custom code to handle the idiosyncrasies of the nested data shown above,
and even then, the output isn’t guaranteed to be clean and consistent enough for an LLM to reason over. A simpler and quicker way is to use a multimodal LLM like OpenAI’sgpt-4o, or its smaller cousingpt-4o-minithat take in images as input. We can use BAML to prompt these models to extract the
relevant entities and relationships from animageof each page of the PDF, rather than writing a ton
of custom code to preprocess the data as text.
This approach of extracting entities and relationships from images is surprisingly effective, in terms
of cost, quality and speed, as we’ll see below! Here’s a visual breakdown of the methodology: Let’s walk through the key steps in the following sections. 1. PDF -> Image This simple FastAPI appis used to transform the PDF data of drugs into a series of PNG images2.
The images are transformed to their base64-encoded string representations, which multimodal LLMs likegpt-4o-minican interpret
well. Because each page of the PDF is represented as a separate image, we can easily scale up this approach to handle
PDFs with far more pages than what’s shown in this example, concurrently processing each page to speed things up. The text data (clinical notes) is left as is, because it’s already in a clean text format that can read by an LLM. 2. Extract structured data using BAML We are now ready to prompt the LLM to extract entities and relationships from the image and text files. In this
section, we’ll break down the BAML prompts that are used for each task: information extraction from images and text. Graph schema The preliminary step, prior to any LLM prompting, is to sketch out a graph schema that we will use to model our domain.
This is what informs our BAML schema design, shown further below. In the PDF data, each drug has a generic name and (optionally) a brand name. Each class of
drugs has a set of side effects, and are ",healthcare data,"unstructured data, graph database, LLMs",The article discusses the use of graph databases and LLMs to structure healthcare-related unstructured data.,Unstructured data is the main focus of the article as it pertains to healthcare data.; Graph database is the technology used for structuring the transformed data.; LLMs are mentioned as a tool for generating structured outputs.
https://medium.com/@mcgeehan/building-a-hybrid-vector-search-database-with-arrow-and-duckdb-07ebc049bc1f,Building a Hybrid Vector Search Database with Arrow and DuckDB | by Thomas F McGeehan V | Medium,"Sign up Sign in Sign up Sign in Building a Hybrid Vector Search Database with Arrow and DuckDB -- Listen Share How we combined HNSW for fast vector search with SQL-based metadata filtering for a next-gen AI database. Vector databases are no longer optional. As embeddings-based applications explode across industries, developers need speed, flexibility, and efficiency when storing, searching, and retrieving high-dimensional vectors. But here’s the problem: Most vector databases force you to choose between raw performance and SQL-powered filtering. That’s why we builtQuiver— a Go-powered, hybrid vector database that delivers the best of both worlds:✅ HNSW for fast, high-recall vector search✅ DuckDB for structured metadata filtering✅ Apache Arrow for efficient, zero-copy data movement With Quiver, you can run complex queries that mix vector similarity with structured constraints — without killing performance. This article isn’t just a high-level introduction. We’re diving deep into the internals — how I integrated Apache Arrow, optimized DuckDB for metadata management, and built a high-speed HNSW index — to create a vector search engine that doesn’t compromise. Let’s get into it. Vector Databases Have a Problem Vector databases are critical infrastructure for AI applications — powering everything from semantic search and recommendation systems to image similarity and anomaly detection. But despite their importance, most solutions come with serious trade-offs. Where Existing Vector Databases Fall Short 🔥 Performance vs. Flexibility Trade-offs → Most vector databases are built for either fast similarity search or rich metadata filtering — not both. If you want speed, you lose SQL-style filtering. If you want SQL filtering, you lose performance. 🔥 Heavy Resource Consumption → Many solutions demand huge memory and CPU overhead to maintain vector indices, making them expensive at scale. 🔥 Operational Complexity → Most vector databases require careful tuning, background maintenance processes, and periodic reindexing to stay performant. 🔥 Integration Challenges → Existing solutions often sit outside an organization’s primary data stack, requiring custom pipelines and workarounds to sync with relational databases and analytical engines. How Existing Solutions Approach the Problem Current vector search solutions take different approaches, each with trade-offs: Why We Built Quiver We wanted a vector database that didn’t force these trade-offs — one that keeps up with the best ANN search engines while supporting rich, efficient metadata filtering. ✅ HNSW for high-speed vector search✅ DuckDB for SQL-powered metadata filtering✅ Apache Arrow for zero-copy, high-performance data movement Quiver avoids the overhead of external databases, scales efficiently, and fits seamlessly into modern AI and analytics stacks. Let’s break down how it works. Under the Hood Quiver is built on three key components, each designed to maximize speed, flexibility, and efficiency: 1️⃣ Vector Index → An HNSW (Hierarchical Navigable Small World) graph for fast approximate nearest neighbor (ANN) search.2️⃣ Metadata Store → A DuckDB-backed SQL engine for structured metadata filtering.3️⃣ Arrow Appender → A zero-copy data pipeline powered by Apache Arrow, keeping everything memory-efficient and fast. These pieces work together to eliminate the trade-offs most vector databases force you to make — allowing queries that combine vector similarity with structured constraints without wrecking performance. Blazing-Fast Search: The Vector Index At the core of Quiver’s vector search engine is HNSW (Hierarchical Navigable Small World) — a graph-based ANN algorithm built for speed and scalability. HNSW is what makes Quiver’s search both fast and accurate, offering logarithmic search complexity while keeping recall high. It works by constructing a multi-layered graph where similar vectors cluster together, drastically reducing the number of comparisons needed for a query. Our Go-based implementation of HNSW is optimized for:✅ Memory efficiency → Minimizing index size without sacrificing recall.✅ High-speed inserts → Batch processing for fast vector ingestion.✅ Precision tuning → Fine-grained control over search parameters for balancing speed vs. accuracy. The HNSW graph is parameterized by several key configuration options: The `HNSWM` parameter controls the maximum number of connections per node in the graph, while `HNSWEfConstruct` and `HNSWEfSearch` control the search breadth during index construction and query time, respectively. These parameters allow for fine-tuning the trade-off between search speed and accuracy. SQL Meets Vectors: The Metadata Store What sets Quiver apart isn’t just fast vector search — it’s fast vector search with real SQL filtering. That’s where DuckDB comes in. Unlike traditional vector databases that rely on key-value stores or embedded document storage, Quiver uses DuckDB, an in-process analytical database built for speed. This gives u",uncategorized,none,Failed to process content.,Failed to extract keywords.
https://github.com/Manirevuri/arangodb-hackathon,GitHub - Manirevuri/arangodb-hackathon,"Navigation Menu Search code, repositories, users, issues, pull requests... Provide feedback We read every piece of feedback, and take your input very seriously. Saved searches Use saved searches to filter your results more quickly To see all available qualifiers, see ourdocumentation. Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. Manirevuri/arangodb-hackathon Folders and files Latest commit History Repository files navigation GraphMind AI: Natural Language Query Agent for CVE Graph Database  Watch the video on Youtube Note: Adding the video here as we ran into playback issues while uploading it to youtube. Project Overview We've developed an AI agent that can process natural language queries on a Common Vulnerabilities and Exposures (CVE) graph database. The graph contains approximately 145,000 nodes and 316,000 edges, allowing for comprehensive vulnerability analysis. We are leveraging claude's MCP integration for creating dynamic UI components. Architecture Diagram  Important Setup Notes Query Tools Our system implements three specialized tools: The ReAct agent is designed to leverage these individual tools to create hybrid query mechanisms for more complex analysis tasks. Getting Started The provided notebook includes sample queries and outputs for each query type (AQL, ArangoSearch, NetworkX code generation, and hybrid queries). Required Environment Variables Performance Considerations Hybrid tool calling  UI Screenshots from Claude Artifacts  About Resources Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. Stars Watchers Forks Releases Packages0 Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. Languages Footer Footer navigation",ai and legal systems,"CVE graph database, AI agent, natural language query",The article discusses an AI tool for analyzing Common Vulnerabilities and Exposures (CVE) data using a graph database.,CVE graph database is the main focus of the article.; AI agent is the tool developed to process natural language queries on the CVE graph database.; Natural language query refers to the ability of the AI agent to understand and respond to human language requests.
https://medium.com/@tomzeppenfeldt/querying-an-erp-using-an-ai-configuration-in-graphileon-c667f21f5b51,Querying an ERP using an AI configuration in Graphileon | by Graphileon — Your path to graphs | Medium,"Sign up Sign in Sign up Sign in Querying an ERP using an AI configuration in Graphileon -- Listen Share I just created a simple though powerful AI configuration in Graphileon to assist users querying their ERP system. What do we feed to the AI? All the AI configuration needs is the model, and a question in natural language.The model is provided through a list of patterns with a short description, like in the image below. The question is provided in natural language. For instance: Which were the 5 most recent purchase orders for ""XYZ"" ? I want to have orderdate, delivery date, supplier , PO number and quantity. The LLM (in our case OpenAI's o3-mini) returns its interpretation, an explanation, the patterns it thinks are relevant, the suggested Cypher statement and the parameters.Note that in this first step, we have not yet provided the full list of properties and their datatypes that are present on the nodes and edges of each pattern. In order to prepare for step 2, we loop through the relevant patterns (see ""relationshipsUsed"" in the picture above), and get for each pattern a random set of datapoints. These datapoints are anonymised, and included in the messages for the second request to the LLM, in which we ask it to review the Cypher statement and use the actual property names and datatypes as they are present in the datapoints. In the final step, Graphileon runs the generated Cypher statement against the database and displays the results. In parallel, an explanation is provided in a Markdown viewer. The functionality described above only takes nine Graphileon functions: -- -- Written byGraphileon — Your path to graphs Graphileon helps business consultants and information analysts to rapidly design and deploy graph-based applications by exploiting the agility of graphs. No responses yet Help Status About Careers Press Blog Privacy Rules Terms Text to speech",ai and legal systems,"AI configuration, Graphileon, ERP system",The article discusses the use of AI in configuring an ERP system using Graphileon.,AI configuration is the central concept of the article.; Graphileon is the tool used for this configuration.; ERP system refers to the system being configured.
https://2024.connected-data.london/speakers/urbashi-mitra/,Urbashi Mitra - Connected Data London 2024,"Have you purchased yours? Buy Your All-Access Pass now & get lifetime access! Existing Attendee?Sign In Urbashi Mitra Gordon S. Marshall Professor in Engineering,
                                

                                
                                    
                                        University of Southern CaliforniaUrbashi Mitra received the B.S. and the M.S. degrees from the University of California at Berkeley and her Ph.D. from Princeton University. She is the Gordon S. Marshall Professor in Engineering at the University of Southern California with appointments in Electrical Engineering and Computer Science About this speaker Dr. Mitra is a Fellow of the IEEE and a Foreign member of Academia Europaea. She is a co-principal investigator of the recently funded National Science Foundation Center on Pandemic Insights. Dr. Mitra was the inaugural Editor-in-Chief for the IEEE Transactions on Molecular, Biological and Multi-scale Communications. Her honors include: the 2024 IEEE Information Theory Society’s Aaron D. Wyner Distinguished Service Award, the 2021 USC Viterbi School of Engineering Senior Research Award, the 2017 IEEE Communications Society Women in Communications Engineering Technical Achievement Award, a 2016 UK Royal Academy of Engineering Distinguished Visiting Professorship, a 2016 US Fulbright Scholar Award,  2012 Globecom Signal Processing for Communications Symposium Best Paper Award, 2012 US National Academy of Engineering Lillian Gilbreth Lectureship, Student Best Paper Award, as co-advisor, at the International Conference on Signal Processing and Communications, Bangalore India 2012, the 2009 DCOSS Applications & Systems Best Paper Award, 2001 Okawa Foundation Award, 2000 OSU College of Engineering Lumley Award for Research, 1997 OSU College of Engineering MacQuigg Award for Teaching, and a 1996 National Science Foundation CAREER Award. Her research interests are in: model-based machine learning, wireless communications, communication and sensor networks, biological communication systems, and the interface of communication, sensing and control. Talks Causal Graph Identification:  optimization, performance bounds and reward optimization Edges Track Review and Unconference Session Proudly supported by  Neo4j, the Graph Database & Analytics leader, helps organizations find hidden relationships and patterns across billions of data connections deeply, easily, and quickly. Proudly supported by  Connect the dots of your data! Ontotext helps enterprises to lower data management costs by up to 30%, enable data fabric architectures, create digital twins, utilize Graph RAG benefits, and take information delivery from days to minutes! Proudly supported by  The vendor of PoolParty Semantic Suite. Graph-based text mining, recommender systems, and data fabric solutions. Proudly supported by  yWorks specializes in the development of professional software solutions that enable the clear visualization of diagrams and networks. Proudly supported by  We’re a cloud tech company that provides organisations around the world with computing infrastructure and software to help them innovate, unlock efficiencies and become more effective. We also created the world’s first – and only – autonomous database to help organise and secure our customers’ data. Proudly supported by  Ultipa builds next-gen graph XAI & real-time database empowering smart enterprises w/ smooth digital transformations. Proudly supported by  Oxford Semantic Technologies (OST) spun out from the University of Oxford and was acquired by Samsung in 2024. OST provides AI software to extract insights from big data, solving issues like medical diagnostics and financial crime. One founder is a BCS Lovelace Medal winner. Proudly supported by  Web3 data platform built on standards. Fluree powers connected, secure, and agile data ecosystems. Proudly supported by  Senzing is the first to deliver real-time, artificial intelligence for entity resolution. Senzing software enables organizations of all sizes to gain highly accurate and valuable insights about who is who and who is related to whom in data. Proudly supported by  We partner with you, and your chosen semantic stack, to liberate your data's meaning from isolated silos. Proudly supported by  All-in-one platform to create AI agents powered by your private data and knowledge.
Make GenAI prototype to production 10 times faster.
We are backed by Y Combinator.
Start free today: https://epsilla.com Proudly supported by  Since 2016 Neural Alpha have delivered cutting edge, sustainability centric Connected Data solutions for blue-chip corporates, financial institutions, Governments and NGOs.  Our bespoke software & data solutions fuse AI, Knowledge Graphs, Taxonomies & other technologies for unprecedented insights. Proudly supported by  Graphwise, born from the merger of Ontotext and Semantic Web Company, empowers enterprises to maximize AI ROI with trusted knowledge graph and semantic A",ai and legal systems,"machine learning, data science, award","The article discusses AI research in the field of data science, with a focus on awards and recognitions.","Machine learning is the central concept of the article as it relates to Dr. Mitra's research.; Data science is implied throughout the text, as the context suggests work in this field.; Award is mentioned multiple times, highlighting Dr. Mitra's achievements."
https://darrendevitt.com/all-fhir-concepts-can-be-explained-simply/,All FHIR concepts can be explained simply – Darren Devitt,"All FHIR concepts can be explained simply FHIR is not rocket science. There’s a quote attributed to Einstein that resonates with me: “If you can’t explain it simply, you don’t understand it well enough.” Writing these posts helps clarify my own understanding of FHIR. When all you have is 300 words there’s no room for complicating things. Here are 9 short posts I wrote over the past year on different FHIR concepts. I learned something new from each one. Let’s start at the very beginning Next, move on to the plumbing Finally, lift the hood on some advanced FHIR features Discussion --- Sign up to my Weekly Email. Check out theVanya FHIR Viewerfor Windows and Mac. Related <span class=""nav-subtitle screen-reader-text"">Page</span> Copyright © 2025 - Darren Devitt Discover more from Darren Devitt Subscribe now to keep reading and get access to the full archive. Type your email… Subscribe Continue reading",healthcare data,"FHIR, healthcare interoperability","The article focuses on FHIR, a healthcare data standard.",FHIR is the primary subject of the article.; Healthcare interoperability is implied through the discussion of FHIR.
https://medium.com/@samschifman/rag-on-fhir-with-knowledge-graphs-04d8e13ee96e,RAG on FHIR with Knowledge Graphs | by Sam Schifman | Medium,"Sign up Sign in Sign up Sign in RAG on FHIR with Knowledge Graphs -- 4 Listen Share In my last post,RAG on FHIR, I introduced some ideas for using FHIR to fuel Retrieval Augmented Generation (RAG). There I went into some detail about what RAG is, but in short it is using some curated data to enrich prompts sent to an Large Language Model (LLM), so that the Artificial Intelligence (AI) can generate a more accurate response. In that article, I also discussed why this makes a difference in healthcare and whyFast Healthcare Interoperable Resources(FHIR) are particularly well suited for RAG. This was a great start to being able to leverage LLMs in healthcare in a responsible way. However, it was only a start. In this article, I will continue to build on those ideas. In particular, I propose to address two limitations of that initial work: Let’s take a closer look at each of these. Healthcare Data is Connected One of the great features of FHIR is that it recognizes that data in healthcare is linked in many ways. For example, a blood pressure measurement is linked to a patient, but it is also linked to the encounter (office visit) in which the reading was taken, the time it was taken, and who took it. FHIR represents this by containing References from one FHIR resource to another. This allows one to fetch all the blood pressure readings for a patient in a nice neat package, and then go back for the other data later. These links are critical to help us understand the real picture of the patient’s healthcare. For example, if that blood pressure was high, we may want to understand what medication changes the doctor made to address that. We can trace the path from the blood pressure to the encounter to the medications ordered at the encounter. But, the real question we probably want to answer is, did the treatment make a difference. To answer this we need another concept, time. Time Matters in Healthcare To figure out if some treatment was successful or not, we need to put that treatment in the context of when it was started and what happened after that. FHIR does not apply any particular ordering to the data. However, most FHIR resources do contain dates that can provide that context. So, you can get all the blood pressure readings and sort them by date and then see if the readings after the encounter in question get better or not. RAG and Connected Data The RAG I discussed in my last article used a Vector Index / Store to capture the meaning of each FHIR resource and allow us to search for them. In my YouTube video,RAG on FHIR, I go into some detail about how this works. However, the Vector Store has no idea about the linking of resources. So, we need a different data structure to handle that. Knowledge Graphs Knowledge Graphs (or generally Graph Data Stores) are very good at capturing this sort of linking. In this context, I am talking about a graph that stores nodes and edges that link nodes, not bar or pie charts. We can visualize a graph like this: Here we have a Knowledge Graph that contains knowledge about the speed of various things. In particular, from this graph we can see that dogs can go 45 MPH and that they are animals, which is different than vehicles. While a car can go 200 MPH, but is not an animal. Knowledge Graphs from FHIR The above graph is a very simple example, but we can use references in FHIR to build a more complex graph about a patient’s healthcare: This is actually only part of the graph created from one FHIR bundle. I again used data generated bySynthea. In this case it is from the data set they make publicly availablehere. What I did was to read each resource, flatten it, and create a node per resource. I then looked in each resource again and pulled out any references to other resources. From these references I created edges to link the nodes. You can see all the code in theJupyter Notebook. Vector Index on Knowledge Graph You might be wondering if I am abandoning Vector Stores completely. The answer is no. When I flatten the FHIR resources, I still create a string representation of them. This time I do that with slightly more sophisticated code, which you can see inGitHub. I then use functionality inNeo4J, the Graph Database I am using, to create a Vector Index of the nodes. You can think of it like projecting the graph into a vector space: We can now turn a question into a vector and find the node that is most likely to answer that question. What about the devourer of all things… Time We now have our connected data in the graph, but we still don’t have any concept of time. What we can do is go back through our resources and pull out the times. In this case I am more interested in just the date part of the time, so I just pulled dates. I then created a node per each unique date and created edges from nodes that contained that date. So, now the graph looks something like this: Each node that contains a date is linked to a node for that date. I make sure that I only create one node per sp",healthcare data,"FHIR, Retrieval Augmented Generation (RAG), Linked Data",The article discusses the use of FHIR for managing healthcare data and its linked nature.,"FHIR is the specific technology used in this context.; Retrieval Augmented Generation (RAG) is the AI concept being applied to healthcare data.; Linked Data, as represented by References in FHIR resources, is a key aspect of the article."
https://medium.com/enterprise-rag/why-lawyers-are-uniquely-suited-to-work-with-llms-bcc66d3dce98,Why Lawyers Are Uniquely Suited to Work with LLMs | by Chia Jeng Yang | Knowledge Graph RAG | Medium,"Sign up Sign in Sign up Sign in Knowledge Graph RAG  Devs and non-technical domain experts build Agentic and RAG-native knowledge graphs  Featured Why Lawyers Are Uniquely Suited to Work with LLMs What This Article Covers: -- 1 Listen Share This article discusses how, despite the fact that lawyers are not engineers, a legally trained mind — when equipped with basic technical knowledge of Large Language Models (LLMs) — can uniquely contribute to their deployment and coordination. Many LLM failures stem from the way AI systems handle language. While impressive, LLMs do not think like humans. Law is one of the few professions built on structuring reasoning and maintaining consistency through language. Lawyers, with the right technical foundation can collaborate with engineers to create expert reasoning and retrieval workflows — charting processes for how LLMs should reason about specific types of legal information. AI offers lawyers the potential to create a programmatic version of themselves — one that retains their expertise but never tires or retires. By understanding LLM search processes like Retrieval Augmented Generation (RAG), lawyers can pinpoint why an LLM fails to retrieve relevant answers and use their semantic expertise to refine its output. Most LLM systems useRetrieval-Augmented Generation (RAG)to pull in external information beyond their training data. This includes searching documents, websites, and other sources before generating an answer. I think of LLMs like an intern. They can follow instructions literally, lack intuitive reasoning, and struggle to connect implicit concepts. They also have a tendency to forget information between interactions.Besides all of these, like an intern with a particular quirk in the way they think, LLMs rely on a single method — semantic similarity — to interpret questions and retrieve relevant information. Knowing this, this approach can be reverse engineered to identify failures. For example, when searching a 300-page contract for “consequential damages,” a lawyer knows: Similarly, LLMs struggle when retrieving conceptually related but substantively material terms. If asked,“What liabilities does the contract impose?”, an LLM may overlook relevant sections if “consequential damages” and “indirect losses” are regarded as non-semantically similar. Lawyers, trained to be sensitive to conceptual equivalencies, can help address these gaps. Let us go in depth into how LLMs retrieve and answer questions in a RAG system so we can start looking at more complex scenarios (and more likely scenarios you may have come across). The basics of RAG that a lawyer would need to know: Chunking, Semantic Similarity, Chunk Filtering Retrieval Augmented Generation (RAG) operates in three critical stages: That’s it — that’s essentially all a simple RAG system does. If I have learnt anything between building developer infrastructure and going through law school, it is that developers, like lawyers, use highly technical terms that belie relatively simple concepts to help enhance job security. In the below image, you can see what a chunk looks like, and you can program the size of the chunks to use. The larger the chunks, the more the information. The smaller the chunks, the smaller the information. Only a certain amount of words can fit into the LLMs context window when they construct the answer. Therefore, the larger the chunks, the less chunks you can fit into the LLM. Variable: Chunk sizeLarge chunks: Better for extracting generalized meaning and insightSmall chunks: Better for extracting specific facts, figures, or statistics The only other thing to know is something called the Top-K filter. This is just a fancy way of saying ‘What is the top number of document chunks we will decide to select to give to the LLM to construct its answer’. Top-K filtering determines how many document chunks an LLM uses to generate an answer. For example, if Top-K = 3, the system selects the three most relevant chunks. A higher Top-K captures more information but risks hallucinations, while a lower Top-K ensures focused answers but may miss key details. One obvious limitation is imagining that you have a question which requires information that is spread across 10 document chunks (i.e. tell me the top 10 clauses that are related to this issue) and the top-k is set to 5, meaning only the top 5 document chunks are retrieved. Variable: Top-kHigher values: More complete responses that incorporate large amounts of informationLower values: More focused, consistent answers For example, consider the query:“What is the recommended dosage of acetaminophen for an adult?” If the system retrieves onlyone chunk, the answer may be misleading. Even worse, if neither chunk explicitly mentions “acetaminophen,” they might not be retrieved at all. Onecommon trickis to ask the LLM to add metadata summaries to each chunk — a summary of the document itself (i.e. imagining that the 2 documents are “Dosage instructions for Acet",ai and legal systems,"Large Language Models (LLMs), Retrieval Augmented Generation (RAG)",The article discusses the unique contribution of lawyers to the deployment and coordination of Large Language Models in legal systems.,Large Language Models are the main focus of the article.; Retrieval Augmented Generation is the AI technology used by LLMs for information retrieval.
https://link.springer.com/journal/10506,"Home | Artificial Intelligence and Law
        ","Artificial Intelligence and Law Overview This journal seeks papers that address the development of formal or computational models of legal knowledge, reasoning, and decision making. It also includes in-depth studies of innovative artificial intelligence systems that are being used in the legal domain, and gives space to studies addressing the legal, ethical and social implications of the use of artificial intelligence in law. It welcomes interdisciplinary approaches including not only artificial intelligence and jurisprudence, but also logic, machine learning, cognitive psychology, linguistics, or philosophy. In addition to original research contributions, this journal welcomes book reviews as well as research notes posing interesting and timely research challenges. Societies and partnerships Journal metrics Latest issue Latest articles Legal text classification in Korean sexual offense cases: from traditional machine learning to large language models with XAI insights Empirical analysis of binding precedent efficiency in Brazilian Supreme Court via case classification R2GQA: retriever-reader-generator question answering system to support students understanding legal regulations in higher education Advancing prompt-based language models in the legal domain: adaptive strategies and research challenges Specialized or general AI? a comparative evaluation of LLMs’ performance in legal tasks Sign up for alerts Get notified when new articles are published. Journal information © Springer Nature B.V. Journal navigation About this journal Articles For authors Search Navigation Discover content Publish with us Products and services Our brands 168.91.255.120 Not affiliated © 2025 Springer Nature",ai and legal systems,"artificial intelligence, legal domain, machine learning",The article discusses the application of artificial intelligence in the legal field.,Artificial Intelligence is the main focus of the article.; Legal domain refers to the specific context of the discussed AI applications.; Machine learning is mentioned as a key technology used.
https://www.openownership.org/en/blog/lessons-from-building-a-prototype-single-search-tool-for-beneficial-ownership-registers/,Lessons from building a prototype single-search tool for beneficial ownership registers | openownership.org,"Lessons from building a prototype single-search tool for beneficial ownership registers Photo byLisa ZoeonUnsplash. Beneficial ownership (BO) data is useful only once it is in the hands of those who need it. However, ourrecent researchwith users of BO data found that lack of access to information is still one of the biggest barriers to its effective use. Our work developing theOpen Ownership Registershowed that there is demand for accessing BO information from multiple countries. Yet, only a few countries publish the bulk BO data required to power such tools. Many countriesaremaking it possible for users to search and explore BO data at a national level. But if you’re conducting an investigation into a company or person, you might not know where to look. Therefore, knowingwhethera register holds specific information andwhichregister holds that specific information could save time and resources in investigations. A little while ago, we had a simple idea – what if there was a place where you could search multiple countries’ BO registers at once? Would that be possible? Would that be useful? We spent some time over the last few months creating and testing a tool to start answering these questions. We wanted to learn: This blog post describes the tool we built in light of these questions, and what we learned in the process. Initial thinking To start thinking about what this might look like, we developed a product vision board to consider our target audience and their needs. We then discussed what kind of product might be able to serve these needs. This exercise helped us make a few key decisions: Identifying sources We came up with a longlist of 33 potential sources of information – BO registers – for the tool based on the Open Ownershipmap. These potential sources either provided some level of public search functionality or were present in commercial tools like Moody’s Orbis database. We then needed to figure out which of these would be accessible. Our criteria were that the source: This left us with a list of nine accessible BO registers we could search by company name or entity ID number, and four we could search by a person’s name. Note: this list is not exhaustive – there may be other registers that we could have connected to, but these were the ones we identified in the time available. The tool With these sources, we designed a simple tool. It has just a search bar and radio button which allows you to choose whether to search for a company name or a person’s name. After inputting the name of a company and selecting ""Company search"", the tool will return the list of sources and number of matches found in the source. For example, searching the company name “Aurubis” returns: It then links to each separate BO register, where the user can choose to go and recreate the search to try to access the actual BO information if they so choose. Similarly, the user can instead choose to search for a person’s name – doing so returns a similar but shorter table of registers, just those that allow a search by a person’s name: Users are then able to go and recreate the search on the relevant registers if they get a match. If you want to try it yourself, the tool is availablehere– you will need to follow the install instructions in the README file. Testing with users After creating the tool, we wanted to get it in front of some real-life users of BO data in order to see if it would be something they found useful. We identified a number of types of users that might find this kind of information helpful, including: We conducted a total of six interviews with relevant users. We asked each interviewee: Following a demonstration of the tool, we subsequently asked: What we learned We spent 11 days of developer time, plus another 20 days on design and testing. Despite the many limitations, we learned some interesting things: All users who tested the tool expressed that there was value in a simple signposting tool. As one bank employee said: However, nobody followed up to ask for access following the demonstration, which would have been a much stronger signal of value. Four of the six people interviewed suggested that adding more sources within the tool would make it more useful, over and above providing information about the beneficial owners of companies or companies owned by the people searched. However, some users did ask to see the data within the tool itself, particularly if it could either visualise BO networks or enable the search of other sources for information retrieved by the initial search. For example, if an initial search for a company name would retrieve the name of a beneficial owner, the user would be able to directly conduct a second search for that beneficial owner. It took 11 total developer days to set up 9 sources and a simple user interface. Of course, there are bugs and issues that could be ironed out with more time, but this is not a difficult technical problem. When the sources are accessible – even if po",federated search,"beneficial ownership, multi-country search, data access",The article discusses the development of a tool for federated search across multiple beneficial ownership registers.,Beneficial ownership is the main focus of the article.; Multi-country search refers to the ability to search multiple countries' registers simultaneously.; Data access highlights the importance of making beneficial ownership data easily accessible.
https://www.occrp.org/en/project/cyprus-confidential/billionaire-roman-abramovichs-company-set-up-fake-superyacht-chartering-scheme-in-apparent-attempt-to-evade-millions-in-taxes,Billionaire Roman Abramovich’s Company Set Up Fake Superyacht Chartering Scheme in Apparent Attempt to Evade Millions in Taxes | OCCRP," Matching Results Get quality reporting directly into your inbox, every week By submitting your email address you agree toOCCRP’s Privacy Policy Billionaire Roman Abramovich’s Company Set Up Fake Superyacht Chartering Scheme in Apparent Attempt to Evade Millions in Taxes People working for Abramovich designed a corporate structure to give the false impression that the billionaire’s yachts were being commercially chartered to third parties, and therefore eligible for tax breaks. James O'Brien/OCCRP Banner:
      James O'Brien/OCCRP Related Articles Leak Unveils Russian Oligarch Abramovich’s $1 Billion Art Collection. Despite Sanctions, It Has Not Been Seized or Frozen. Credit Suisse Banked Abramovich Fortune Held in Secret Offshore Companies Abramovich’s Secret Football Payments May Have Breached Financial Fair Play Rules Reported by A company ultimately owned by the Russian billionaire Roman Abramovich set up a fake superyacht-leasing business in Cyprus in an apparent attempt to evade millions of euros in tax, leaked files and correspondence obtained by OCCRP reveal. Between 1999 and 2010, the former Chelsea Football Club owner assembled a fleet of superyachts worth around $1.2 billion, including the world’s longest at the time, Eclipse, which boasted two helipads and a swimming pool that could turn into a dance floor. The yachts spent part of the year sailing in European waters. Operating them was expensive, and under EU law, value-added tax was due on everything needed to keep them in service — fuel, staff, port fees, maintenance, and more. But these costs were exempt from the EU tax for vessels used for commercial purposes. To claim the exemption, people working for Abramovich devised a complex scheme in which the luxury vessels were leased to what looked like independent customers paying to go on a cruise for a week or two. In reality, the companies hiring the superyachts were owned by an offshore trust whose beneficial owner was Abramovich. Cypriot tax officials later found that the yachts were not in fact used commercially, and in 2012 the country’s tax authority ordered Abramovich’s company to pay 14 million euros ($18.5 million) to cover the unpaid tax. Lawyers for the company tried to overturn the order in court, but the appeal was finally dismissed last year. Representatives for Abramovich did not respond to questions about whether the outstanding tax bill had been settled. Billionaire Roman Abramovich. Leaked emails from people working for Abramovich, discovered by The Bureau of Investigative Journalism and the BBC and shared with OCCRP, reveal an explicit intention to deceive tax inspectors. “Our structure must as clearly as possible separate the different parties so that an investigator checking on our operation would see it as a legitimate structure,” a director of Abramovich’s company wrote in an email in 2005, when the scheme began. “But we all have to recognise that a determined investigator could eventually discover this is an in-house structure with the possible consequences that would entail.” A Leak of Secretive Cypriot Documents A Leak of Secretive Cypriot Documents The leaked files originate from Cypriot corporate service provider MeritServus, whose managing director, Cypriot chartered accountant Demetris Ioannides, was sanctioned by the U.K. government in 2023 for setting up “murky offshore structures” for Abramovich. (Ioannides did not respond to a request for comment.) The documents were obtained by the whistleblower group Distributed Denial of Secrets and initially shared with OCCRP and The Guardian. This investigation is part of Cyprus Confidential, a global investigative collaboration led by the International Consortium of Investigative Journalists (ICIJ) and Paper Trail Media. The email is a “smoking gun for tax artificiality,” Rita de la Feria, a professor of tax law at the University of Leeds, told reporters at OCCRP’s partner outlets the BBC and the Guardian. “It does appear that this structure is artificial, that it is set up in order to pass off private consumption of the yachts as business expenditure.” Asked by reporters whether the tax had been paid, the head of Cyprus’ Audit Office said only, “The matter will be examined by our Office.” Lawyers for Abramovich denied “any allegation that [Abramovich] had or ought to have any knowledge of, is personally responsible for and/or is personally liable for any alleged deception of any government authority in order to evade payment of taxes which were lawfully due or for any other purpose.” OCCRP has not seen any leaked emails about the yachts to or from Abramovich himself. But the emails frequently refer to the billionaire and include many exchanges between service providers and one of Abramovich’s close friends, who was also a business associate of his. A 'Tax-Efficient' Structure for Superyachts 2005 was a big year for Roman Abramovich. In April, Chelsea F.C., the English football team he had purchased two years earlier, won",financial crime technology,"superyacht, tax evasion, offshore trust",The article discusses a financial crime involving the use of complex structures to evade taxes.,Superyacht is the asset involved in the financial crime.; Tax evasion is the criminal activity described.; Offshore trust is the method used to hide ownership.
https://www.occrp.org/en/project/the-azerbaijani-laundromat/the-contract-factory-inside-danske-bank-estonias-money-laundering-machine,‘The Contract Factory’: Inside Danske Bank Estonia’s Money Laundering Machine | OCCRP," Matching Results Get quality reporting directly into your inbox, every week By submitting your email address you agree toOCCRP’s Privacy Policy ‘The Contract Factory’: Inside Danske Bank Estonia’s Money Laundering Machine Investigators allege 19 bankers laundered more than $1.6 billion of illicit funds, some of it from a huge Russian tax evasion scheme and a Facebook phishing scam. Banner:
      Alexander Welscher/dpa/Alamy Live News Related Articles Azerbaijani Couple, Including President’s Cousin, Owns Properties Worth Millions in London and Ibiza Jet-Setting DJ — and Cousin of Azerbaijan’s President Aliyev — Accused of Receiving Millions In Suspicious Funds Agreed Statement Reported by Skype user Maxim.Canning was a veritable one-stop-shop for anyone looking to launder illicit funds. For a fee, he would not only sell you a company registered on an offshore paradise island, but also help you to disguise who really owned it. If you needed to transfer suspect money, he could make fake contracts to give it a semblance of legality, or rapidly funnel the cash through dozens of offshore accounts. “Maxim” could even find you proxy directors to add to company paperwork — for a price. “We have for 1000 euros per piece a Kazakh until 2022,” he wrote to one client in 2014, after the documents they had provided for a different director didn’t check out. Estonian investigators say Maxim’s real name was Jevgeni Agnevštšikov, and he was one of 19 people on Danske Bank Estonia’s foreign banking team who they believe laundered more than US$1.6 billion of illicit funds for their clients. The money allegedly originated in eight schemes in Russia, Azerbaijan, the U.S., Iran, Switzerland, and Georgia, as well as a scam that defrauded Facebook out of nearly $100 million. It flowed through Danske Bank Estonia accounts between 2007 and 2015, when the bank was at the center of one of Europe’s largest ever money laundering scandals. Late last year, investigators presented a 160-page report to the 19 bankers detailing the accusations against them. The document, obtained by Estonia’s Eesti Ekspress and shared with OCCRP and Danish newspaper Berlingske, outlines the charges that prosecutors expect to file in the coming months, before the case goes to court. Only 15 are expected to face charges, however, as two were found not to have been involved and two provided evidence. Investigators allege Agnevštšikov’s bosses, Juri Kidjajev and Erik Lidmets, started selling off-the-books services in 2005, when they worked for the Estonian branch of Sampo Bank. They allegedly continued after Danske Bank bought Sampo two years later, recruiting other trusted bankers to join their team. When Danske Bank closed its foreign banking division in 2015, several of these bankers moved to other lenders in Estonia, where investigators say some continued to launder money for up to four more years. Lidmets declined to comment and reporters were unable to reach Kidjajev. Andres Simson, a lawyer for Agnevštšikov and another of the accused bankers, declined to comment while the case is ongoing, saying only: “I can confirm that my defendants deny any guilt."" Philippe Vollot, chief administrative officer at Danske Bank, said the bank was cooperating with authorities in the investigation. “Combating financial crime and money laundering is a key priority for Danske Bank. Overall, we are now in a different position with respect to combating financial crime and money laundering than when the situation developed in Estonia,” he said. State prosecutor Maria Entsik declined to comment on the contents of the report, which she said should not be disclosed before trial. She said police would soon send the full case file to prosecutors, so they could decide whether to take it forward. “To date, a total amounting to 10 million euros in assets of suspects and third parties enriched by the proceeds of crime have been seized to secure the confiscation requirements of the state,” she added. ‘Long Live Photoshop’ Conversations cited in the investigative document show how the bankers helped their clients to avoid running afoul of Danske Bank’s anti-money laundering controls. In one Skype chat, banker Mikhail Murnikov advised user thunderball1111 on how to fill in the details of a proxy director to ensure it would be accepted by the bank’s Moscow offices, including adding fake seals and signatures. “Long live Photoshop,” he joked. In another conversation from September 2014, Murnikov contacted someone going by the Skype name Richard-zorge77 with what he described as a “mega-proposal.” “If the contracts go so difficult for you and not very beautifully. We have an additional service. We make contracts with stamps and signatures for our and every other bank. And whatever other documents. The price list is 1500 euros per company.” It is not clear from the investigators’ report if Richard-zorge77 accepted the “mega-proposal,” but the following April, Murnikov sent him another message. ",financial crime technology,"money laundering, offshore accounts, proxy directors",The article discusses the use of financial technologies for money laundering activities.,Money laundering is the central topic of the article.; Offshore accounts are mentioned as a method used in money laundering.; Proxy directors are discussed as potential actors in money laundering schemes.
https://www.occrp.org/en/project/the-azerbaijani-laundromat/the-raw-data,The Raw Data | OCCRP," Matching Results Get quality reporting directly into your inbox, every week By submitting your email address you agree toOCCRP’s Privacy Policy The Raw Data Related Articles ‘The Contract Factory’: Inside Danske Bank Estonia’s Money Laundering Machine Azerbaijani Couple, Including President’s Cousin, Owns Properties Worth Millions in London and Ibiza Jet-Setting DJ — and Cousin of Azerbaijan’s President Aliyev — Accused of Receiving Millions In Suspicious Funds Data Analysis by Amy Guy, Friedrich Lindenberg, Lion Summerbell The database below contains nearly 17,000 payments made to and from the four core UK-registered companies that made up the Azerbaijani Laundromat. Their bank accounts were held at the Estonian branch of Danske Bank and were obtained by Berlinske, an OCCRP partner. They cover the period form June 2012 until the end of 2014. The records provide a rare snapshot of where the money came from, where it went, who was involved and how it was spent. Not all details can be clearly seen, but the overall picture is crystal clear. The stories in the Azerbaijani Laundromat investigation are based in part on this data. OCCRP is sharing the full database here so that readers can do their own searches. DISCLAIMER: Please note that the database may contain legitimate business transactions, and that the presence of any name in this dataset does not necessarily imply any intentional wrongdoing. If you have any questions or interesting findings, please reach out to OCCRP at[email protected]. Join the fight.Hold power to account. Join the fight.Hold power to account. Support from readers like you helps OCCRP expose organized crime and corruption around the world. By donating, you’ll be directly supporting investigative journalism as a public good. You’ll also gain access to exclusive insights and benefits. Get quality reporting directly into your inbox, every week By submitting your email address you agree toOCCRP’s Privacy Policy",organized crime analysis,"Azerbaijani Laundromat, Danske Bank Estonia, money laundering","The article discusses an investigation into money laundering activities, specifically the Azerbaijani Laundromat case.",Azerbaijani Laundromat is the name given to the investigation subject.; Danske Bank Estonia is the bank involved in the case.; Money laundering is the illegal activity being investigated.
https://discuss.opensanctions.org/,OpenSanctions,"Get help using the OpenSanctions database, API and bulk data Research & Development Announcements General Community Use Cases & Case Studies Support & Questions Powered byDiscourse, best viewed with JavaScript enabled",financial crime technology,"OpenSanctions, API, bulk data",The article discusses a financial crime technology tool named OpenSanctions.,OpenSanctions is the main subject of the article.; API and bulk data suggest this tool provides programmatic access to its functionality.; 
https://blog.opencorporates.com/2025/02/13/getting-started-with-the-opencorporates-api/,Getting started with the OpenCorporates API: A beginner’s guide – OpenCorporates,"Blog Getting started with the OpenCorporates API: A beginner’s guide Have you ever needed to look up information about companies across different countries? Whether you’re in business, government, or research, the OpenCorporates API provides a powerful way to access data about millions of companies worldwide. In this guide, we’ll walk through everything you need to know to start using the API. What is OpenCorporates? OpenCorporates is the largest open database of company information in the world. Our API lets you programmatically access: Getting started 1. Get your API key Before you can start making requests, you’ll need an API key: 2. Understanding the basics The API endpoint isapi.opencorporates.com. All requests should use HTTPS, and your API token should be included as a query parameter. Here’s the basic structure: To make a request from this endpoint you can use: 3. Your first API request Let’s start with something simple – looking up a company. Here’s how to find information about a specific company: This URL breaks down as: The response will include: Common use cases and examples 1. Searching for companies To search for companies by name: The search is designed to be flexible: You can refine your search with parameters like: 2. Looking up company officers Find information about company directors and officers: 3. Understanding corporate networks Explore company relationships and corporate hierarchies: Best practices Working with the results The API returns JSON by default. Here’s what a basic company response looks like: Common challenges and solutions Next steps Now that you understand the basics, you might want to: Resources and support Remember, the OpenCorporates API is a powerful tool for accessing company data, but it’s important to use it responsibly and in accordance with theterms of service.Start with simple queries, test thoroughly, and gradually build up to more complex integrations. For help with particular use cases, feel free tospeak with us today. For more information Learn more about how OpenCorporates’ data can help you understand corporate structures and manage risk. Reach out for a demo or explore our services.  Share this: Like this: Related Read more articles You Might Also Like Who owns what? Making sense of LLC ownership in US real estate Reflections of a product manager: What you should look for in company data At last: corporate events for everyone Leave a ReplyCancel reply Twitter Updates OpenCorporates tweets Our sites Recent Posts Email Subscription Enter your email address to subscribe to this blog and receive notifications of new posts by email. Email Address Sign me up! Search this blog Discover more from OpenCorporates Subscribe now to keep reading and get access to the full archive. Type your email… Subscribe Continue reading",financial crime technology,"OpenCorporates API, company data, search","The article discusses a tool used for accessing company data, which is often utilized in financial crime analysis.",OpenCorporates API is the main focus of the article.; Company data refers to the type of information provided by the API.; Search is a key function discussed in the article.
https://www.buzzsprout.com/242645/episodes/16799543,The sharing of suspicion,The Dark Money Files The Dark Money Files The sharing of suspicion Share episode SARs! Search and rescue services? Subject access requests? Sudden acute respiratory syndrome? No - suspicious activity reports! That's what we are talking about this time. And there's a lot more to them than you might think. Send us a text Support the show Follow us on LinkedIn athttps://www.linkedin.com/company/the-dark-money-files-ltd/on Twitter athttps://twitter.com/dark_filesor see our website athttps://www.thedarkmoneyfiles.com/ The Dark Money Files + Follow,financial crime technology,"suspicious activity reports, SARs","The article discusses suspicious activity reports, a common term in financial crime technology.",Suspicious activity reports are the central topic of the article.
https://www.unodc.org/unodc/en/data-and-analysis/tip-studies.html,Study on Trafficking in Persons and Smuggling of Migrants in the Context of the Displacement caused by the War in Ukraine,"United Nations Office on Drugs and Crime Study on Trafficking in Persons and Smuggling of Migrants in the Context of the Displacement
            caused by the War against Ukraine Trafficking in persons, smuggling of migrants and the war against Ukraine The war against Ukraine has resulted in millions of people being displaced internally and outside the country since February 2022. This UNODC study, launched in February 2025, examines the risks and incidence of trafficking in persons and smuggling of migrants in the context of the displacement caused by the war against Ukraine, and the implications for policy and practice. The research analyses the evolution of trafficking in persons and smuggling of migrants during the years 2022 to 2024, based on relevant literature, statistics, a survey in 2023 of over 1,600 Ukrainians and non-Ukrainians displaced from Ukraine and key informant interviews in Ukraine, Germany, Poland and Switzerland. The study finds that the incidence of smuggling of migrants and trafficking in persons in the context of the displacement remain relatively low as of the time of publication. The refugee response in Europe – maintaining visa-free entry for refugees from Ukraine and facilitating rapid access to temporary protection or equivalent legal residence statuses – largely prevented smuggling of migrants, in a situation where over 6.7 million people from Ukraine sought refuge abroad. Targeted information campaigns, increased efforts to identify victims of trafficking, law enforcement cooperation and other anti-trafficking policies and actions by state and non-state actors, in Ukraine and in countries of transit and destination, may have further strengthened resilience. These measures also show significant potential for adaptation and application to other refugee displacements and migration movements in Europe and elsewhere. Nevertheless, precarious employment and accommodation situations in host countries make refugees from Ukraine vulnerable to trafficking for forced labour. The involvement of Ukrainian refugee women in prostitution and sex work in host countries also presents indications of vulnerability and sexual exploitation, including online. Read the full study here inEnglishorUkrainian. Download the special points of interest here inEnglishorUkrainian. Data Data and analysis for the UNODC Global Report on Trafficking in Persons is availablehere. UNODC’s data on detected victims of trafficking in persons as well as on trafficking offenders can be downloaded from the UNODC data portal,dataUNODC. The UNODC Observatory on Smuggling of Migrants can be accessedhere",organized crime analysis,"trafficking in persons, smuggling of migrants",The article discusses the study on organized crime related to trafficking and smuggling of people displaced by the war in Ukraine.,Trafficking in persons is the main focus of the study.; Smuggling of migrants is also a key aspect of the study.
https://www.which.co.uk/news/article/scam-empire-inside-the-275m-fraud-call-centre-operations-aP3Kc4c9HWd7,Scam Empire: inside the $275m fraud call-centre operations - Which?,"Suggested searches Suggested searches Scam Empire: the $275m scam call-centres flogging bogus investments Two major call centre operations are running commercial-scale investment frauds worth at least $275m, affecting 32,934 victims across 33 different countries to date, according to a new report. TheScam Empireinvestigation is a collaborative effort by the Organized Crime and Corruption Reporting Project (OCCRP), Swedish Television and various media outlets, based on 1.9 terabytes of leaked data – including 20,000 screen recordings and over 20,000 hours of audio calls – shared by an anonymous source. Here we take you through the findings, explaining why fraud victims can't wait any longer for tech companies to face stronger regulations and sharing simple tips to help you spot an investment scam. Sign up for scam alerts Our emails will alert you to scams doing the rounds, and provide practical advice to keep you one step ahead of fraudsters. The scam call centres Two call centre operations were found to be running essentially the same scam, using similar training materials, scripts, and contracts with vendors. One is the 'A.K. Group' with at least three offices in Tbilisi, the capital of Georgia, which employed around 85 people as of April 2024. A second operation had offices in Israel, Bulgaria, Ukraine, Spain and Cyprus, employing at least 480 people as of August 2023. The call centres pushed dozens of branded investment platforms, offering investments in cryptocurrency, stocks like Tesla, or other appealing financial products. Reporters forScam Empirecontacted 182 people listed as having 'invested' money, more than 90% of whom (166 people) said they had been the victims of a scam. Even more compelling is the 20,000 hours of recorded phone calls, giving 'every indication that both operations were designed to scam large numbers of people'. Call centre staff reportedly used false identities, forged paperwork, and deceptive online advertising to entice investors. The report details staff swapping jokes and gloating about victims with each other on Telegram and Skype, as well as lavish parties and performance bonuses to celebrate their 'wins'. An investigator from Spain’s Mossos d’Esquadra, the Catalonian police, said call centres can be more lucrative than trafficking drugs because the margins are higher and the risks of being caught are much lower. Investment fraud tactics TheScam Empirereport found that major tech companies profited from online adverts that pushed victims into these rogue investment schemes. Which? has warned for years that fraudsters use social media and online advertising as hunting grounds. We exposed Google and Microsoft's Bing for lettingscammers take out via paid-for advertsto peddle fake investments in 2021 andcalled out Facebookfor letting the same scammers repeatedly take out advertising in 2022. More recently, official bank data revealed that more than half of bank transfer scams recorded in 2023originated on Facebook, Instagram or WhatsApp, all of which are owned by parent company Meta. Remote access and screen sharing apps were also cited in theScam Empirereport, something wewarned about in 2020. Although AnyDesk is a legitimate provider, OCCRP found that call centre scammers commonly used it to access vast amounts of personal information from their victims’ computers. AnyDesk said it is 'tirelessly working to prevent' the use of its software by scammers and worked closely with law enforcement to fight against scam call centres, including adding a warning for first-time connections from suspicious accounts, though it acknowledged 'a large portion of these attacks involve social engineering where a victim is coached around these automated countermeasures we put in place'. In response to the report's findings, Google said: 'We expressly prohibit scam ads on our platforms and take swift action to suspend the offending advertiser’s account when applicable.' Meta said: 'It is against our policies to run ads that promote or facilitate scams. We stand ready to review and take action if we find violations of our policies.' Following the money – why don't banks stop payments? Victims were encouraged to make payments using cryptocurrencies or less strictly regulated digital payment providers and coached to bypass security checks from banking staff. The Guardianreported that Revolut, which received a UK banking licence last year, was the most mentioned (linked to 119 customers out of 403 listed in an internal Georgian call centre spreadsheet), followed by Kroo (involved with 50 victims). Last year, we expressed our concerns about how oftenRevolut is mentioned in fraud reportsto Which?. Victims are often encouraged to move money to the digital e-money provider first before being transferred again to cryptocurrency exchanges and other accounts that are difficult to trace. The most recentWhich? survey of fraud victimsfound that nearly one in five people who inadvertently sent money to scamme",organized crime analysis,"investment fraud, call center operations",The article discusses a large-scale organized investment fraud conducted through call center operations.,Investment fraud is the main focus of the article.; Call center operations are identified as the means for carrying out the fraud.
https://www.occrp.org/en/project/scam-empire/scam-operations-relied-on-third-party-marketing-companies-for-steady-stream-of-potential-victims,Scam Operations Relied on Third-Party Marketing Companies for Steady Stream of Potential Victims | OCCRP," Matching Results Get quality reporting directly into your inbox, every week By submitting your email address you agree toOCCRP’s Privacy Policy Scam Operations Relied on Third-Party Marketing Companies for Steady Stream of Potential Victims Two professional scam call-center operations depended on affiliate marketing firms to supply them with contact details of people who had clicked on ads for phony investment platforms. The marketers were only paid if the victims — some of whom went on to lose their life savings — were ensnared. James O'Brien/OCCRP Banner:
      James O'Brien/OCCRP Related Articles Huge Ecosystem of Unregulated Payment Providers Helps Scammers Collect Victims’ Money Diamonds, Dior and Dubai Vacations: The Luxurious Lives of Georgia's Call-Center Scammers Everything You Need to Know About 'Scam Empire' Reported by It was Tuesday, and that meant payday. Logging into a series of chat groups on encrypted messenger app Telegram, Ben — not his real name, but an alias — reminded the marketers he was helping to oversee that it was time to claim their earnings. “Share invoice plz,” he wrote in one group. “Please do it as fast as possible,” he wrote in another. “We need to send u money.” As invoices poured in, Ben posted an animated GIF from the 1970s Disney cartoon movie Robin Hood, showing the title character dressed as a beggar, asking for a few coins. The invoices were for leads that the marketers had provided to Ben and his colleagues. In marketing, the term ‘lead’ usually refers to a potential client. But these leads were much more sinister. These were potential victims. A screenshot showing ""Ben""'s chat with the affiliate marketers he manages. A screenshot showing ""Ben""'s chat with the affiliate marketers he manages.  Ben and his colleagues worked for a group of call centers running a massive scam operation. The agents who worked there spent their days hoodwinking victims into believing they would make great returns if they invested in cryptocurrencies or other financial products on their platforms. This was a smokescreen — in reality, in the vast majority of cases reviewed by reporters, their money would simply vanish. In order to keep the con going, the scammers needed a steadily incoming stream of potential victims. An unprecedented leak from the heart of a merciless investment scam industry that stretches across the globe reveals how they get them: An ecosystem of marketing companies, known as affiliate marketers, serves up contact details in return for lucrative commissions. The 1.9-terabyte leak that forms the basis of this investigation was obtained by Swedish Television (SVT) and shared with OCCRP and 30 international media outlets. It reveals the inner workings of two groups of call centers: One based in Georgia, and another much larger operation with at least seven offices in Israel, Bulgaria, Ukraine, Spain, and Cyprus. The leaked records show that the marketers that supplied these scammers with leads were richly rewarded. Every lead who ended up making a deposit with the scammers was worth between $200 and $2,350, depending on which country they were from. Victims from wealthier nations like the Netherlands, Sweden, or Belgium were worth more. Records found in the leak show that, in the first seven months of 2024, affiliate marketers working with Ben’s operation — the Israeli/European group — were paid at least $7.3 million for leads. Over the same time period in 2023, they earned more than $10.3 million. About the 'Scam Empire' Investigation About the 'Scam Empire' Investigation Scam Empire is a collaborative investigation by OCCRP, Swedish Television (SVT), and 30 other media partners from multiple countries. Based on 1.9 terabytes of leaked data obtained by SVT, reporters expose two groups of call centers, based in Israel, Eastern Europe, and the country of Georgia, whose employees have convinced at least 32,000 people across the world to make “investments” totalling at least $275 million. The source of the leak pointed to the apparent impunity of the scammers as the motivation for sharing the data in a statement shared with SVT: “Fraudsters can operate almost openly, without anyone stopping them. Almost none of these crimes ever gets resolved because of the difficulties in investigating crimes across borders. Many of the criminals use online phone, chat, and email services to stay anonymous and are based in countries where authorities refuse to cooperate.” “Investigating even one of these scams would take tremendous effort for the police, and a single scammer often commits several of these crimes every day. On top of that, thousands of scammers are doing this daily. Without real international change, this will never go away.”]] Many of the details about the affiliate marketers come from this operation, because the leak contained hundreds of thousands of files relating to the work of its back-office staff, like Ben. The records reveal how Ben and the rest of the marketing mana",organized crime analysis,"scam operation, affiliate marketing",The article discusses a large-scale scam operation relying on affiliate marketing companies.,Scam operation is the main subject of the article.; Affiliate marketing is the method used by the scammers to obtain potential victims.
https://github.com/DAD-CDM/dad-cdm-tsc/blob/main/DAD-CDM-Key-Findings-202502.md,dad-cdm-tsc/DAD-CDM-Key-Findings-202502.md at main · DAD-CDM/dad-cdm-tsc · GitHub,"Navigation Menu Search code, repositories, users, issues, pull requests... Provide feedback We read every piece of feedback, and take your input very seriously. Saved searches Use saved searches to filter your results more quickly To see all available qualifiers, see ourdocumentation. Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. Footer Footer navigation",general tools,"navigation menu, search code, repositories","The article discusses a tool related to searching within repositories, which is a common feature in software development.",Navigation menu is the primary interface for accessing different features.; Search code refers to the functionality of finding specific pieces of code within repositories.; Repositories are the storage locations for the codebase.
https://bods-data.openownership.org/source/gleif_version_0_4/,Beneficial ownership data analysis tools,"Beneficial ownership data analysis tools GLEIF - BODS version 0.4(2025-03-11) TheGlobal Legal Entity Identifier Foundation(GLEIF) is tasked with supporting
the implementation and use of
the_Legal Entity Identifier (LEI).
GLEIF “enables smarter, less costly and more reliable decisions about who to do business with” by providing
“open, standardised and high quality legal entity reference data” to uniquely identify companies. This open
data includes itsLevel 2 Dataon who owns whom. The corporate ownership data collected and published openly by GLEIF is not beneficial ownership data as it
does not identify the natural persons at the end of ownership chains but Open Ownership has mapped a combination
of Level 1 and Level 2 Data toversion 0.4 of our Beneficial Ownership Data Standardin order to analyse and learn more from layering this data on top of beneficial ownership data sources. On Github, you can see how Open Ownership hasmapped, transformed and republishedthis data in line with theBeneficial Ownership Data Standard. We have also published adata use guidewith information about how we have represented change over time and how specific fields have been mapped. These mappings are currently a work in progress and should be treated with caution. This dataset is published
under theCreative Commons CC0 1.0 Universal (CC0 1.0) Public Domain Dedication licence. Downloads and links to hosted databases Tables relationship_statement relationship_recorddetails_interests relationship_source_assertedby relationship_annotations entity_statement entity_annotations entity_recorddetails_identifiers entity_recorddetails_addresses entity_source_assertedby Sample data from `relationship_statement` table. Sample data from `relationship_recorddetails_interests` table. Sample data from `relationship_source_assertedby` table. Sample data from `relationship_annotations` table. Sample data from `entity_statement` table. Sample data from `entity_annotations` table. Sample data from `entity_recorddetails_identifiers` table. Sample data from `entity_recorddetails_addresses` table. Sample data from `entity_source_assertedby` table.",beneficial ownership,"LEI, GLEIF, Open Ownership Data Standard",The article discusses the analysis of beneficial ownership data using tools provided by GLEIF and Open Ownership.,"Legal Entity Identifier (LEI) is a key concept in this article, as it's provided by GLEIF.; GLEIF is the organization tasked with supporting the implementation and use of LEIs.; Open Ownership Data Standard is used for mapping and analyzing beneficial ownership data."
https://eiti.org/using-eiti-data,Using EITI data | EITI,"Secondary header menu Main navigation Using EITI data Examples of how EITI disclosures are analysed and used to address key challenges and inform policies on natural resource governance. Using EITI data EITI disclosures provide vital insights into the governance of natural resources, offering a foundation for accountability, transparency and better decision-making. EITI data can help governments, civil society and other stakeholders identify trends, address challenges and seize opportunities for reform. The examples below illustrate how data from EITI implementing countries has been used, by analysing specific data points relevant to each country’s unique context. In this way, EITI data can shine a light on key issues, sparking debates grounded in trusted data, and shaping policies that promote sustainable and equitable resource management. For more information on using EITI data, or to suggest other examples to include, contact the EITI data team at[email protected]. Licensing Analysing licensing processes to evaluate efficiency, identify risks and ensure transparent and fair allocation of natural resource rights. Mali: License processing times Zambia: Comparing licensing rules vs practice Contracts Examining contract terms to assess compliance, uncover discrepancies and ensure agreements benefit both governments and citizens. Sicomines: How the EITI in DRC helped secure 4 billion in additional revenue Beneficial ownership Scrutinising beneficial ownership information to uncover hidden interests, prevent corruption and promote accountability in the extractive sector. Digging deeper: How Armenia is leading the way in beneficial ownership transparency Oil sales and revenue projections Tracking oil sales and revenue projections to assess market trends, improve fiscal planning and ensure revenues are accurately accounted for. Republic of the Congo: Oil revenue projections Nigeria: Reforming extractive revenue collection Nigeria: Oil sales vs market price Production and exports Monitoring production and export data to evaluate resource management, detect irregularities and ensure revenues match extraction levels. Republic of the Congo: Oil revenue projections State-owned enterprises Assessing the performance and transparency of state-owned enterprises to identify governance gaps, improve oversight and enhance public accountability. Côte d’Ivoire: Crude oil for natural gas swaps Democratic Republic of the Congo: Pioneering reporting on state-owned extractive companies Revenue sharing Monitoring revenue sharing and allocations to ensure that resources are distributed fairly and in line with legal frameworks. Iraq: Oil revenue sharing Subscribe to our newsletter Primary footer menu Secondary footer menu Contact Content Use Policy Unless otherwise noted, you may republish our content for free. Please give credit to our source (for web, with direct link, for print with EITI International Secretariat, eiti.org). Social Media",federated search,"EITI data, natural resource governance",The article discusses the use of EITI data for federated search across natural resource governance.,EITI data is the primary focus and subject matter of the article.; Federated search is implied through the analysis and comparison of data points from various countries.
https://mweiti.gov.mw/index.php/reports/details/761,Report Details | Malawi Extractive Industry Transparency Initiative | MWEITI,"Breadcrumb Beneficial Ownership Study Report The study report examines the legal, institutional and regulatory framework governing Malawi beneficial ownership disclosure. It identifies gaps and challenges related to accessibility of beneficial ownership information by various stakeholders, including competent authorities and law enforcement agencies in Malawi. It also provides recommendations on implementing beneficial ownership disclosure in accordance with EITI International standard, 2023. News Mining Industry in Malawi The mining industry of Malawi, includes a number of gemstones and other minerals. The… Inadequate Power a Setback to Mining Industry in Malawi Minister of Energy, Ibrahim Matola, says that Malawi has inadequate power for mining… Govt to rake in K238bn in Two Mining Deals Signed Malawi is set to make about K238 billion from the decision of giving a go-ahead to… Government, Mining Companies Sign Mining Development Agreements Government through Ministry of Mining and Ministry of Finance has finally signed… Covid-19 Hits Extractive Industry Hard The year 2020 has been a very challenging one for the extractive industry, a report by… Contact Us Useful Links Key Stakeholders Resources Subfooter ©MWEITI. Site developed byIdias Corporation Ltd",financial crime technology,"beneficial ownership, mining industry, Malawi","The article discusses the financial aspect of Malawi's mining industry, focusing on beneficial ownership disclosure.",Beneficial ownership is the main topic of the report.; Mining industry in Malawi is the specific context of the discussion.; Malawi is the geographical location where these events occur.
https://globalenergymonitor.org/projects/global-energy-ownership-tracker/,Global Energy Ownership Tracker - Global Energy Monitor,"About Get Involved Coal Oil & Gas Renewables & Other Power Heavy Industry Regional Special Projects Coal & Steel Oil & Gas Renewables & Other Power Heavy Industry Regional Special Projects News & Reports Newsletters Global Energy Ownership Tracker The Global Energy Ownership Tracker provides information on the chain of ownership for various energy projects. The data maps each level of the chain from the direct owner (as in, the lowest-level identified owner in the chain of ownership) up to their highest-level ultimate parents (e.g., corporations, investment firms, and governments). Ownership links are reported with the percentage of ownership, including owners that have controlling interest as well as those with minority, non-controlling interests (if over a threshold of 5% ownership). This asset ownership data set covers nine of GEM’s trackers: Details about how each tracker identifies their assets and what methodologies they follow can be found on each of their respective pages of the GEM website linked above. The Global Energy Ownership Tracker is updated after each new release of these underlying trackers. The most recent release of this data was in May 2025 and contains ownership data for Coal Plants, Oil and Gas Plants, Bioenergy Plants, Iron and Steel Plants, Coal Mines, Iron Ore Mines, and Natural Gas Pipelines. To learn about the various components of each GEM tracker, readAbout GEM’s Trackers. To receive notifications from this project, please sign up for ourmailing list. If you have questions about this project, pleasecontactthe Project Manager, Anna Mowat. Modal Modal content",financial crime technology,"energy projects, ownership data, chain of ownership","The article discusses a technology used for tracking the ownership of various energy projects, which can be used to prevent financial crimes related to these projects.",Energy projects are the focus of the discussed technology.; Ownership data is the specific information provided by this technology.; Chain of ownership refers to the levels of ownership in the energy projects.
https://www.moodys.com/web/en/us/kyc/resources/insights/how-ai-is-enhancing-ubo-discovery-and-support-reporting-requirements-globally.html,How AI is Transforming Beneficial Ownership Discovery,"Blog 6 ways AI is enhancing UBO discovery and complex investigations for human decisions -interview with an MLRO As the financial industry continues to experience the transformative effects of artificial intelligence (AI) and Augmented Intelligence on its risk management frameworks, there is no better time to review and anticipate AI, whether it involves machine learning (ML), deep learning (DL), or Generative AI (GenAI).AI technologies can help Compliance teams perform tasks requiring human intelligence across numerous processes. These technologies can continuously enhance and augment the accuracy and speed of human-led decision-making.Compliance tools powered by AI technologies can support key processes across the customer lifecycle, including: They can also contribute to prevention and detection capabilities while maximizing the use of data available to Financial Institutions (FI).This article draws from a series of exchanges between Olivier Morlet, a money laundering reporting officer (MLRO) and member of the Global Coalition to Fight Financial Crime (GCFFC), and Moody’s Industry Practice Lead, Francis Marinier.They review six impacts of AI on two key areas - regulatory compliance – specifically around UBO discovery - and social responsibility: 1. Improved data analysis and pattern recognition for UBO discovery AI algorithms can analyze vast amounts of complex ownership data to identify and understand connections and form patterns. Natural language processing also enables relevant information to be processed and extracted from large volumes of unstructured text in seconds. When applied to beneficial ownership, this can help indicate hidden ownership structures. This allows reporting officers to uncover deliberately tenuous or obscured connections between entities and individuals that would be difficult to detect through human analysis alone.The changes introduced by the latest generation AI/ML can transform financial crime, possibly leading to faster standardization of use cases, such as customer screening.The development of unsupervised-training-based programs, which include algorithms and neural networks of parameters, is increasingly being applied in the industry for tasks such as detection, scoring, and investigation.Early AI methodologies built on supervised model training were often used alongside traditional rule-based systems. However, there is a growing shift towards using unsupervised models to identify patterns, similarities, and/or anomalies within groups of aggregated data while helping to reduce false positives and optimize overall efficiency. 2. Automated beneficial owner search and discovery In Moody’s recentstudy into Entity Verification, 46% of respondents identify improving data quality and accuracy as a key challenge in the field AI-enabled solutions can automate searching for and identifying beneficial owners from multiple data sources. Machine learning models can be trained to recognize indicators of beneficial ownership in corporate filings, shareholder documents, and other records. They can also help calculate ownership percentages within connected, circular ownership structures.AI can extract data from relevant corporate registries, standardize it, and utilize entity resolution techniques to determine when different records refer to the same entity, facilitating entity consolidation. It can streamline data collection and integrate information to generate comprehensive ownership structure maps, establishing connections between entities, such as shared addresses, directorships, and other factors.AI can significantly enhance transparency concerning beneficial ownership when worldwide BO registers lack consistency in completeness, format, and accessibility. Harnessing AI alongside a commitment to data completeness and accuracy is quickly emerging as a powerful tool for transparency to assist all who are in the fight against financial crime. 3. Social network analysis (SNA) AI-powered social network analysis (SNA) examines the relationships between entities and individuals to uncover potential beneficial ownership connections among many other links between subjects of interest. This approach helps visualize complex ownership structures, identify central nodes that may represent actual beneficial owners, and develop robust, extended network investigations. Since the early 2010s, these extensive network investigations have increasingly utilized the ‘follow the money’ principle to trace financial connections.Today, SNA can support the identification of Ultimate Beneficial Owners (UBOs) in the following 5 ways:1. Mapping complex ownership structuresSNA allows for the visualization and analysis of complex ownership networks. By representing entities as nodes and ownership relationships as edges, SNA can map intricate corporate structures and reveal: This visual representation can make tracking ownership chains to UBOs more efficient.2. Identifying key players and influencersSNA techniques",financial crime technology,"AI, UBO discovery, data analysis",The article discusses the application of AI in financial crime detection and UBO (Ultimate Beneficial Owner) discovery.,"AI is the central topic as it's being used to enhance financial crime detection.; UBO discovery is a key focus area, as the article discusses AI algorithms analyzing ownership data for this purpose.; Data analysis and pattern recognition are significant aspects of the article, as they are discussed in relation to UBO discovery."
https://oecdstatistics.blog/2019/07/04/the-adima-database-on-multinational-enterprises/,The ADIMA database on Multinational Enterprises,"Recent Posts Most Used Categories  The ADIMA database on Multinational Enterprises By Graham Pilgrim (Graham.PILGRIM@oecd.org), Statistics and Data Directorate (OECD) Multinational Enterprises (MNEs) have been at the forefront of changes in the global economy over the last few decades, as trade and investment barriers have been removed and transportation and communication costs have declined. In a world of global value chains, understanding MNEs – where they are, how they operate, and where they pay taxes – has never been more important. However, surprisingly few official statistics are currently available on individual MNEs. To fill this gap the OECD has begun to develop a new database – theAnalytical Database on Individual Multinationals and Affiliates (ADIMA)– using a number of open “big data” sources that can provide new insights on individual MNEs and their global profiles. What is ADIMA ADIMA has four components: This information already covers 100 of the world’s largest MNEs, and more will be addedin future releases. What does ADIMA tell us on taxes? At an aggregate level, ADIMA shows that in 2016 the 100 MNEs covered in the the database (ADIMA-100) generated nearly $10 trillion in revenues (almost 20% of global GDP), earned $730 billion in profits and paid $185 billion in taxes. But you can also drill down and get more targeted information. For example, although the average Effective Tax Rate (ETR) of the ADIMA-100 was about 25%, it was significantly lower for MNEs producing computers and electronics, and pharmaceuticals, who have substantial intangible assets that they can locate in lower-tax economies (Figure 1). Figure 1: Dispersion of effective tax rates for the ADIMA-100 Note: Low refers to ETR values less than 23%, Medium refers to ETR values between 23% and 33%, and High refers to ETR values greater than 33%. These values were chosen so that a third of the ADIMA-100 population was present in each classification. Where are firms physically located? The physical register provided by ADIMA describes how MNEs structure their physical operations across countries. Here ADIMA’s innovative tools and variety of data sources go beyond the information typically available in company reports (Figure 2), enabling deeper analysis and a mechanism to help profile firms and their affiliates in national and international statistical business registers. For example, companies’ annual reports show that 74 of the ADIMA-100 have a physical presence in the United Kingdom, with an additional 11 MNEs identified using complementary sources (e.g. Legal Entity Identifier, Website Hyperlink Graphs, Server Security Certificates and WikiData), bringing the number of ADIMA-100 MNEs operating in the United Kingdom up to 85. What about MNEs’ digital presence? Physical presence may not reflect digital presence, that is, and in particular for firms whose only penetration into markets is through country-specific websites (i.e. no physical presence). This matters, especially for statistics on highly digitalised MNEs, as the provision of digitised services blurs the traditional line between companies with a foreign presence and those that trade across borders which may affect the comparability of international data on trade and national income. Digital channels are comparable in scale to physical channels: the ADIMA Digital Register captures 20,000 websites while the ADIMA Physical Register captures 26,000 subsidiaries. In smaller countries the digital presence is often more important: for example, only 10 of the ADIMA-100 are physically present in Estonia but a further 19 mainly “digitalised” companies have an electronic presence (Figure 2). Figure 2: Coverage by source for OECD countries by ADIMA-100 MNEs Looking at another interesting example, annual Reports for Alphabet, Google’s parent company, show subsidiaries in two OECD countries but ADIMA’s physical and digital registers record subsidiaries and/or national websites in all OECD countries. For any given domain name, advertising revenue may be recorded as either a domestic or a cross-border transaction. The choice may depend on whether the country-specific site has been legally registered in that country. Next Steps The OECD plans to collaborate with interested official statistical agencies to improve both national statistics on MNEs and ADIMA data. The collaborations should also consolidate the tools developed in ADIMA, extend its coverage of MNEs, improve its methods and incorporate new data sources. The measure explained The OECD Analytical Database on Individual Multinationals and Affiliates (ADIMA) is a new data framework offering information on both the physical and digital presence of MNEs by country. It combines information from traditional sources such as companies’ Annual Reports with newly emerging sources such as the Legal Entity Identifier, Website Hyperlink Graphs, WikiData, OpenStreetMap and Server Security Certificates. Where to find the underlying data? Furt",financial crime technology,"MNEs, taxes, Effective Tax Rate","The article discusses a database (ADIMA) that provides insights on individual Multinational Enterprises, with a focus on their tax profiles.","MNEs are the main subject of the article.; Taxes, particularly Effective Tax Rate, are key aspects discussed in relation to MNEs."
https://oecdstatistics.blog/2025/02/21/monitoring-multinational-enterprises-how-the-oecd-and-unsd-are-harnessing-open-data/,Monitoring multinational enterprises: How the OECD and UNSD are harnessing open data,"Recent Posts Most Used Categories  Monitoring multinational enterprises: How the OECD and UNSD are harnessing open data by Graham Pilgrim (graham.pilgrim@oecd.org) and Eugene Chang (eugene.chang@oecd.org), OECD Statistics and Data Directorate Multinational Enterprises (MNEs) are key actors in the global economy. In 2023, the top 500 MNEs generated over USD 21 trillion in revenues, greater than the  combined GDP of the European Union. They play an increasing role in our lives, whether you are purchasing a new car, asking Generative AI for recipe inspiration, or streaming your favourite song. However, despite their monumental scale, the cross-border nature of their activities means National Statistical Offices (NSOs) only hold information relating to their jurisdiction. Confidentiality concerns often prevent the sharing of this information, making it difficult to see the global picture. The OECD-UNSD Multinational Enterprise Information Platform To overcome this, the OECD and United Nations Statistics Division (UNSD) developed the Multinational Enterprise Information Platform (MEIP), relying solely on publicly available information. The Platform is updated annually, with the third release, covering the period to 31stDecember 2023, now available. The Platform gathers information on the world’s 500 largest MNEs by market capitalisation, facilitating a comprehensive view of their physical and digital presence, and provides a one-stop-shop for data on MNEs and their global networks. In total the platform matches over 130,000 subsidiaries and 120,000 websites to the 500 MNEs. A key benefit of the Platform is the ability to drill down into an individual MNE, without confidentiality concerns. For example, you might want to distinguish the international presences of Nvidia and Intel. Starting with Intel, we can identify 94 subsidiaries, with 31 in the United States and the rest spread over 26 other jurisdictions. Their digital presence is even more dispersed, with 133 websites across 51 jurisdictions. Meanwhile, Nvidia have a narrower physical presence (13 identified subsidiaries in 7 jurisdictions) and digital presence (79 websites across 29 jurisdictions). But this is only the beginning:Explore the dashboardfor insights on the companies that interest you. MNEs in the news Numerous elections, events, and megatrends made 2024 an eventful year. Our updated news database usesGDELT– a database of roughly 200 million worldwide news articles using text analytics to extract information relating to companies, locations, individuals and topics – to map news articles to the top 500 MNEs. In total, over 4 million articles mention one of our 500 MNEs explicitly, roughly 2% of the articles in GDELT. Combined, these MNEs receive more mentions than any individual country. One of the top trending topics of 2024 was Artificial Intelligence, with over 150,000 articles – interestingly, 34% of these articles also mentioned one of our 500 MNEs, demonstrating their important roles in AI development and use. The importance of our 500 MNEs varies widely by jurisdiction. For example, of the 2.6 million articles mentioning the United States just over 10% also mention one of our 500 MNEs. This may not be surprising, given almost half our companies are headquartered in the United States. However, this number is higher for China (12%) and India (11%), suggesting that although a significant number of MNEs in our sample may not be headquartered there, these two countries play a noteworthy role in the functioning of these MNEs. Figure 1.Importance of 500 MEIP MNEs by jurisdiction This news database also allows users to examine individual companies, for example, looking for spikes in media coverage which could correspond to mergers, acquisitions, or controversy. One such example is Abu Dhabi National Oil Company (ADNOC) which announced its agreement to purchase Covestro in October 2024. This transaction represented a significant investment by the United Arab Emirates in a German Headquartered company.  What’s next The Multinational Enterprise Information Platform (MEIP) is a robust foundation on which data can be linked and new insights built. However, this is an ongoing project, and our work is not finished. This year we integrated additional data fromthe Danish business registry, which has significantly enhanced our understanding of the activities of the 500 largest MNEs within Denmark. The registry makes available email addresses and MEIP looks for connections using the domain of each email. For example, Spotify Denmark ApS (Registry Number:3385348) has a contact email ofcopenhagen@spotify.com, meaning MEIP is now able to match this company to Spotify AB (Figure 2). Figure 2. Connecting Spotify AB to Spotify Denmark ApS using the Danish business registry Our implementation of this approach demonstrates that relatively low-cost methods, which do not break confidentiality requirements or require the implementation of a public beneficial ownership ",organized crime analysis,"multinational enterprises, global economy, data platform","The article discusses a data platform designed to monitor the activities of multinational enterprises, which can be associated with organized crime and financial irregularities.","Multinational Enterprises (MNEs) are key players in global economy and can be involved in various illicit activities.; The data platform is specifically designed for MNEs, making it relevant to organized crime analysis.; The platform provides a comprehensive view of the physical and digital presence of these enterprises, which can help identify suspicious activities."
https://www.taxobservatory.eu/publication/the-end-of-londongrad-ownership-transparency-and-offshore-investment-in-real-estate/,The End of Londongrad? Ownership transparency and Offshore Investment in Real Estate - Eutax,"The End of Londongrad? Ownership transparency and Offshore Investment in Real Estate This paper studies the impact of beneficial ownership transparency in the British real estate market. In an effort to reduce illicit investment following the invasion of Ukraine, the UK government announced a new law in 2022 requiring offshore companies that owned domestic real estate to identify their ultimate owners in a public register. Using a difference-in-difference framework, we find that new property purchases by companies registered in tax havens fell relative to those made via non-havens, a result consistent with transparency raising the costs of illicit investment. These declines persist even after dropping tax havens favored by Russians, suggesting that the reform drove the decline, rather than sanctions. We do not find strong evidence of price effects nor substitution into ownership through suspicious domestic companies. While the policy does appear to have been effective at deterring some anonymous investment into the British property market, incomplete implementation led some clients to still successfully shield their ownership information, implying scope for better design and enforcement in the future. This project has received funding by the European Union The EU Tax Observatory is an independent research laboratory hosted by the Paris School of Economics. ©EUTO 2021",financial crime technology,"beneficial ownership transparency, offshore investment, real estate",The article discusses the use of financial crime technology to combat illicit real estate investments.,"Beneficial ownership transparency is the central concept as it aims to reduce anonymous investment in real estate.; Offshore investment is the focus of the study, specifically in relation to tax havens and their impact on the UK market.; Real estate is the asset class under scrutiny for potential illicit activities."
https://www.kharon.com/brief/outbound-investment-rules-china-hong-kong,New U.S. Outbound Investment Rules Target China. How Can You Assess Risk? | Kharon  ,Products Solutions Data The Brief Company Privacy Policy Terms of Use Patents Copyright ©2025 KHARON ,legal systems,"privacy policy, terms of use, patents",The article discusses legal aspects related to the company's products and services.,Privacy policy is a common term in articles discussing user data protection.; Terms of use are legal agreements between a company and its users.; Patents are intellectual property rights that protect inventions.
https://www.gao.gov/products/gao-25-107403,Illicit Finance: Treasury's Initial Safeguards for Allowing Access to Information on Corporate Ownership | U.S. GAO,"U.S. Government Accountability Office Breadcrumb Illicit Finance:Treasury's Initial Safeguards for Allowing Access to Information on Corporate Ownership Fast Facts U.S. companies aren't usually required to publicly name the people who own them. This lack of transparency can be attractive to criminals laundering money or hiding illegal activities. In 2024, the Treasury Department started to require that some companies report owner identity information to its Financial Crimes Enforcement Network. Treasury also recently began granting a few law enforcement agencies access to this data. Treasury has been developing oversight policies to protect the data from unauthorized use. In future annual reports, we will be monitoring Treasury's efforts to share and protect this data.  Highlights What GAO Found Beneficial owners are individuals who, directly or indirectly, own or control a certain percentage of, or exercise substantial control over, a company or other legal entity. The Financial Crimes Enforcement Network (FinCEN) collects and shares information about beneficial owners to help safeguard the U.S. financial system from illicit use and to support law enforcement investigations, among other purposes. In December 2023, FinCEN adopted a rule that allows authorized users (such as federal agencies engaged in national security, intelligence, or law enforcement activity) to access this information if they establish data-safeguarding procedures for it. GAO found that the rule incorporated all the Corporate Transparency Act's requirements for protecting the security and confidentiality of beneficial ownership information. In spring 2024, FinCEN launched the first phase of its five-phase process to grant access to beneficial ownership information by selecting six federal agencies for a pilot, partly to test its IT system. To be approved for access, FinCEN required each agency to sign a memorandum of understanding specifying safeguards they would implement to protect the data. Agencies also had to submit an initial report describing safeguarding procedures and certify they complied with the rule's protection requirements. As of October 2024, four of the six agencies had submitted the necessary documents (see table). Federal Agencies Approved to Access Beneficial Ownership Information, as of October 29, 2024 Agencies granted access Federal Bureau of Investigation Internal Revenue Service-Criminal Investigations U.S. Postal Inspection Service U.S. Secret Service Agency staff with access 100 System searches conducted Nearly 1,700 Source: Financial Crimes Enforcement Network. | GAO-25-107403 In September 2024, FinCEN began the second phase, enabling about 200 additional federal agencies to request access to beneficial ownership information. Based on feedback from the pilot, FinCEN revised its memorandum of understanding to clarify certain compliance requirements. FinCEN also created a procedure for reviewing and granting agency requests for information. Agencies must still sign a memorandum of understanding, report on their safeguarding procedures, and certify compliance with information protection requirements. FinCEN officials told GAO they have been developing policies and procedures for overseeing users' access and safeguarding practices. This oversight is to include conducting annual audits, monitoring how users search the data, and reviewing reports submitted by authorized users. FinCEN plans to assess the need for additional oversight mechanisms as the access program is fully implemented. GAO will continue to monitor FinCEN's implementation of its oversight policies and procedures. Why GAO Did This Study The Corporate Transparency Act, enacted in 2021, requires certain legal entities to report their beneficial ownership information to FinCEN. This requirement supports U.S. efforts to prevent bad actors from concealing or benefiting from ill-gotten gains through shell companies or other opaque ownership structures. The act required FinCEN to adopt a rule to safeguard this information from unauthorized use. The Corporate Transparency Act also includes a provision for GAO to determine whether FinCEN's safeguards, procedures, and use of beneficial ownership information, as established in its access rule, are consistent with requirements of the act. This report is the first in a series of seven annual reports. This report examines (1) whether FinCEN's access rule included Corporate Transparency Act requirements for protecting the security and confidentiality of beneficial ownership information, (2) the extent to which FinCEN granted agencies access to beneficial ownership information in compliance with the act, and (3) FinCEN's oversight of agencies' access to and use of the information. GAO reviewed the Corporate Transparency Act and FinCEN's beneficial ownership information access rule; analyzed FinCEN's policies, procedures, and other documents related to the access program; and interviewed FinCEN and other agency offi",financial crime technology,"beneficial ownership, Financial Crimes Enforcement Network (FinCEN), data protection",The article discusses the U.S. government's efforts to combat financial crimes by improving transparency of corporate ownership.,"Beneficial owners are a central concept in the article as they relate to identifying individuals who can potentially hide illegal activities.; Financial Crimes Enforcement Network (FinCEN) is the main entity responsible for collecting and sharing beneficial ownership information.; Data protection is an important aspect of the article, as it discusses measures taken to safeguard this sensitive information."
https://hoeringsportalen.dk/Hearing/Details/69602,Høringsdetaljer - Høringsportalen," Høring Høring over forslag til lov om ændring af lov om Det Centrale Virksomhedsregister, selskabsloven og forskellige andre love (Ændring af adgangen til oplysninger om reelle ejere som følge af 6. hvidvaskdirektiv) Dokumenter © 2025 - Civilstyrelsen Tilgængelighedserklæring",legal systems,"law change, company law, real owner access",The article discusses a proposed law change related to company and other laws.,Law change is the main topic of the article.; Company law is one of the specific laws mentioned.; Real owner access refers to the aspect of providing information about real owners.
https://ubm.se/publikationer/publikationer/2025-03-13-kunskapsrapport---avancerade-angrepp-mot-valfardssystemen,Kunskaps­rapport – Avancerade angrepp mot välfärds­systemen - Utbetalningsmyndigheten,"Till innehåll Dölj navigering Utbetalningsmyndigheten Kunskaps­rapport – Avancerade angrepp mot välfärds­systemen Utbetalnings­myndigheten presenterar en lägesbild som visar att avancerade angrepp mot välfärds­systemen framför allt sker genom oseriösa företag, identitets­missbruk och manipulation av folkbokföring och intyg. Relaterade filer:",organized crime analysis,"fraudulent companies, identity abuse, social security","The article discusses advanced attacks on welfare systems, which are primarily carried out through fraudulent companies, identity abuse, and manipulation of social security.",Fraudulent companies is the main actor in the advanced attacks.; Identity abuse is a method used in these attacks.; Social security refers to the targeted system.
https://senzing.com/daniel-silva-prudential-senzing-global-user-conference/,Daniel Silva of Prudential on Insurance Fraud Detection,"Daniel Silva of Prudential Discusses Insurance Fraud Detection At Senzing Global User Conference “We’re no longer trying to stay 10 steps ahead of the criminals – we’re trying to stay ahead of the machines. It’s machine versus machine at this point,” declares Daniel Silva, director of data analytics atPrudential, addressing the audience at the Senzing 2024 Global User Conference. His assessments cut to the heart of why traditional fraud prevention approaches need to evolve to meet today’s challenges in the life insurance and annuity sectors. Silva, who also serves as a board member for theInstitute of Data Scienceat New Jersey Institute of Technology (NJIT), has deep expertise in financial fraud detection and data analytics. In his presentation, he illuminates how the complexity of insurance products, combined with legacy data systems and emerging AI threats, creates unique challenges for modern-day fraud prevention.  The Complex Web of Insurance Data Silva explains that understanding fraud in the insurance space requires first understanding the life insurance products themselves. He demonstrates how even a basic life insurance policy creates a complex network of relationships. Each policy involves multiple entities – policy owner, insured and beneficiaries – that must be carefully tracked and verified. The challenge becomes even more complex when dealing with legacy data. Prudential faces unique data hurdles as a company celebrating 150 years in operation. Among the challenges is ensuring legitimate beneficiaries from new and existing policies receive their intended benefits while preventing fraud. To address these challenges, Prudential has implemented comprehensive verification systems that use a multi-layered approach to help establish what Silva calls “business truth” in an increasingly complex data environment. Three Faces of Insurance Fraud Silva breaks down insurance fraud into three distinct categories, each requiring different detection approaches: The Data Challenge: Legacy and Siloed Systems For established insurers, the data infrastructure itself can become an obstacle to fraud detection. Having data in multiple places can make it harder to spot patterns and identify suspicious activity across products and services. Prudential processes multiple unique record changes monthly, and each change introduces new data that must be resolved and verified. To manage this constant flow of updates, Prudential must validate and connect information across disparate systems while maintaining regulatory compliance and data privacy requirements. The Rise of AI-Powered Threats The emergence of AI and machine learning has sparked a new urgency to intensify fraud prevention efforts. Silva details how fraudsters are now using AI to systematically harvest security questions from financial institutions and generate convincing voice samples to bypass authentication. The threat extends beyond voice authentication. Criminals are using AI for credential-stuffing attacks, where machine learning systems attempt to identify patterns in security questions and responses. This has led to a new arms race in the financial sector, where institutions must deploy their own AI systems to detect and prevent these automated AI-powered attacks. A Modern Defense: Entity Resolution and Real Time Analysis This evolving threat landscape makes real time entity resolution crucial. “As a new entity is being introduced into the system, you need to really quickly understand who they are,” Silva emphasizes. Prudential’s approach combines traditional verification methods with sophisticated systems that can identify suspicious combinations of addresses, phone numbers and social security numbers across its various platforms in real time. The company also uses entity resolution systems to maintain accurate contact information for beneficiaries beyond just their name so benefits reach the intended recipients. By easily resolving data across multiple data sources, Prudential can maintain accurate beneficiary information even as circumstances change over decades. By understanding the complex relationships between entities in real time,insurers can better detect and prevent fraudbefore significant losses occur. Technology likeSenzing entity resolutioncan provide the foundation for successfully addressing these challenges, helping companies like Prudential maintain their commitment to protecting customers while adapting to new threats. In his presentation, Silva makes it clear that successful fraud prevention in today’s insurance industry requires a sophisticated understanding of both traditional insurance products and emerging technologies. As fraudsters deploy ever-more advanced tools, the industry must evolve its approach to data management and fraud detection while maintaining its fundamental promise to protect customers’ financial security. Video Highlights 00:42 Understanding insurance policy structure and relationshipsEach policy creates a complex",financial crime technology,"insurance fraud, data analytics, legacy data",The article discusses the challenges and solutions in detecting insurance fraud using advanced technologies.,Insurance fraud is the main focus of the article.; Data analytics is the method used to address the issue.; Legacy data is a unique challenge faced by the company.
https://graphaware.com/resources/streamlining-criminal-assets-confiscation/,Streamlining Criminal Assets Confiscation | GraphAware,"Connected data analytics platform. Explore how state-of-the-art graph technology can redefine intelligence analysis. Transitioning to connected data analytics is a rewarding journey.We’re here to help. Leverage intelligence-led policing with mission-critical graph analytics capabilities. Gain capabilities to act quickly, stop fraud and protect your clients and your business. Enable automation, learn about bad actors and their networks, and leverage predictive strategies. New use cases, features, and live demos designed to make analysts’ lives easier. Our research, philosophy and case studies, all wrapped up in books and papers. Discover GraphAware Hume’s features, demos, and graph technology insights. Review our in-depth user guides and technical documentation to ensure flawless operations. A globally-recognised graph technology company. Want to work at GraphAware?Check our open positions. Get in touch with us.Drop us a line. Join our mission to help everyone to find the truth in data. Meet us all around the world. Streamlining Criminal Assets Confiscation Home/Resources/Case Studies/Streamlining Criminal Assets Confiscation BACKGROUNDThe Agency is a member of the financial crime investigation taskforce which investigates criminal wealth by targeting the proceeds and instruments of crime in both Australia and overseas. BEFOREData siloed in disparate systems   |   Manual, time consuming investigation   |   4 weeks of work by a skilled team AFTERSingle view of ownership structures across many data sets   |   Easily explore multi-hop connections and patterns in relationships   |   ± 1 hour of work by a skilled analyst Our client is a law enforcement agency, with operational priorities including investigating complex crime, and criminal assets confiscation, while exploiting advanced technology to support Australia’s national interests. Targeting the criminal economy and confiscating criminal assets is crucial to disrupting and deterring organised criminal activity and delivering maximum impact. Criminal Assets Hidden Behind Complex Legal Company Structures In order to confiscate assets effectively, the taskforce must first identify assets that can be seized and then conduct a legal process for the confiscation. Increasingly, organised crime organisations are utilising legal company structures to mask the ownership of their assets. These company structures can also include complex beneficial ownership structures to further mask ownership of assets. In addition to this, there are dozens of data sources that contain both the beneficial ownership structures and the various asset types including real estate data, shareholdings, motor vehicles and much more. The combination of the utilisation of complex legal company structures as well as the number of asset data sets required means that identifying assets that can be seized is a manual, time consuming task. The identification of assets for one case alone was in some cases taking a skilled team over a month to complete. This elapsed time hampered taskforce’s ability to quickly and effectively complete their mission of seizing assets. Unified View of Company Structures and Asset Data GraphAware HumewithNeo4jis perfectly suited to storing and analysing complex company ownership chains. Rather than analysing company ownerships manually, graphs are able to represent the network of company ownerships almost instantaneously. In addition, Hume is perfectly suited to creating a single view of intelligence across many data sets. The combination of these two technologies means that Agency analysts can instantly uncover complex company structures, and the assets that are attached to them in one single tool. The Agency team utilised GraphAware Hume platform to load the required company structure data and asset data sets to the graph. Then, using Hume actions, the team developed queries that would allow analysts to select an entity of interest and query the database for the company structures they were associated with. Finally, with the company structures returned to the canvas, the team developed Hume actions that presented assets that were attached to the companies on the canvas. Identifying Confiscatable Assets in 1 Hour Instead of 4 Weeks Utilising the GraphAware Hume platform, the Agency team were able to build a cutting edge solution for the financial crime investigation taskforce. Analysts are now able to instantly identify company ownerships and the assets attached to them. One of the core functions of the Agency, the identification of potential assets was now a process that took 1 hour, instead of up to 4 weeks. This vast improvement in the speed to identify and analyse assets meant that financial crime investigation taskforce were able to identify significantly more assets, and have a better understanding of the complex company structures that are being utilised by organised criminals. Graphs have been a game changer for our agency and its mission. — Coordinator Data An",financial crime technology,"graph technology, criminal assets confiscation, investigation",The article discusses the use of graph technology in financial crime investigation and asset confiscation.,Graph technology is the central concept for analyzing data in this context.; Criminal assets confiscation is the primary focus of the agency mentioned.; Investigation is the main activity enabled by the use of graph technology.
https://graphaware.com/law-enforcement/,Connected Data for Criminal Intelligence Analysis,"Connected data analytics platform. Explore how state-of-the-art graph technology can redefine intelligence analysis. Transitioning to connected data analytics is a rewarding journey.We’re here to help. Leverage intelligence-led policing with mission-critical graph analytics capabilities. Gain capabilities to act quickly, stop fraud and protect your clients and your business. Enable automation, learn about bad actors and their networks, and leverage predictive strategies. New use cases, features, and live demos designed to make analysts’ lives easier. Our research, philosophy and case studies, all wrapped up in books and papers. Discover GraphAware Hume’s features, demos, and graph technology insights. Review our in-depth user guides and technical documentation to ensure flawless operations. A globally-recognised graph technology company. Want to work at GraphAware?Check our open positions. Get in touch with us.Drop us a line. Join our mission to help everyone to find the truth in data. Meet us all around the world. AcceleratedCriminalIntelligence Home/Law Enforcement Accelerated criminal Intelligence analysis — Law Enforcement Agency, Data Science Lead —  Western Australia Police Force, Senior Tactical Intelligence Analyst Connected Data Analytics Platform Single Viewof Truth Connect data from siloed sources into a single view of truth, structured exactly as your organisation understands it. Geospatial and Temporal Analysis Investigate data visualised in relevant locations, and see how data changes over time. AutomatedAlerting Continuously monitor patterns of interest, and alert colleagues so that your team never misses anything. See Hume in action Key Features for Law Enforcement THE GRAPHAWARE HUME ADVANTAGE Protect communities with the power of connected data. Unify and enrich datasets from siloed sources, enabling analysts to act faster against criminals and their networks. A Single Tool For All Analysis Everything an analyst requires in just one tool — from data ingestion to comprehensive report generation. Investigate Immediately All your data points are pre-linked and resolved, automatically connected into a knowledge graph for immediate exploration. Add New Data During Investigations Ingest data on demand, perform additional enrichment, query external systems, and much more — supporting analysts’ creativity with near unlimited flexibility. Uncover Unknown Unknowns Discover patterns and relationships with multi-hop connection analysis, and query them geo-temporally. Simple & Visual Query Creation Use visual query-building with no coding expertise required. Expand your link diagram and get to actionable intelligence faster. Share Intelligence Easily Set up workflows to distribute insights to various stakeholders seamlessly using snapshots and annotations. Predictive Power Enable co-offending network analysis and link prediction using context-rich network data, powered by graph data science and AI. Real world use cases Powering the criminal intelligence analysis of law enforcement agencies.  Three Steps to Intelligence-Led Policing The knowledge graph approach to intelligence-led policing bridges data silos, providing a comprehensive structure to data and a single view of intelligence. Graph adoption Support Your Graph Journey Adopting graph technology doesn’t need to be complicated. With our proven process and expertise, we will support you throughout the whole journey. Book Your Live Demo Seeing is believing.Schedule a live walkthrough withone of our specialists. Discover the power of Knowledge Graphs in Criminal Intelligence Analysis. Connected data analytics platform. Explore how state-of-the-art graph technology can redefine intelligence analysis. Transitioning to connected data analytics is a rewarding journey.We’re here to help. Leverage intelligence-led policing with mission-critical graph analytics capabilities. Gain capabilities to act quickly, stop fraud and protect your clients and your business. Enable automation, learn about bad actors and their networks, and leverage predictive strategies. New use cases, features, and live demos designed to make analysts’ lives easier. Our research, philosophy and case studies, all wrapped up in books and papers. Discover GraphAware Hume’s features, demos, and graph technology insights. Review our in-depth user guides and technical documentation to ensure flawless operations. A globally-recognised graph technology company. Want to work at GraphAware?Check our open positions. Get in touch with us.Drop us a line. Join our mission to help everyone to find the truth in data. Meet us all around the world.",organized crime analysis,"connected data analytics, graph technology",The article discusses the use of connected data analytics and graph technology in organized crime analysis.,Connected data analytics is the central concept for investigating organized crime.; Graph technology is the tool used to unify and enrich datasets from siloed sources.
https://graphaware.com/blog/aml-investigations-detecting-risk-transactions/,AML Investigations: Detecting High-Risk Transactions with Network Analytics,"Connected data analytics platform. Explore how state-of-the-art graph technology can redefine intelligence analysis. Transitioning to connected data analytics is a rewarding journey.We’re here to help. Leverage intelligence-led policing with mission-critical graph analytics capabilities. Gain capabilities to act quickly, stop fraud and protect your clients and your business. Enable automation, learn about bad actors and their networks, and leverage predictive strategies. New use cases, features, and live demos designed to make analysts’ lives easier. Our research, philosophy and case studies, all wrapped up in books and papers. Discover GraphAware Hume’s features, demos, and graph technology insights. Review our in-depth user guides and technical documentation to ensure flawless operations. A globally-recognised graph technology company. Want to work at GraphAware?Check our open positions. Get in touch with us.Drop us a line. Join our mission to help everyone to find the truth in data. Meet us all around the world. AML Investigations: Detecting High-Risk Transactions with Network Analytics October 4, 2024 · 6 min read Home/Blog/AML Investigations: Detecting High-Risk Transactions with Network Analytics In the ever-evolving landscape of financial crime, Anti-Money Laundering (AML) and Know Your Customer (KYC) procedures are critical tools for detecting suspicious activity. As criminals become more sophisticated, so too must the methods used to AML investigations and uncover illicit financial networks. One high-profile case that illustrates the complexity of modern financial crime is theAzerbaijani Laundromat, a large-scale money-laundering operation that funnelled billions of dollars through a network of shell companies. In the a recently published webinar, our experts demonstrate howconnected data analysis tools, can transform the fight against financial crime, enabling institutions to shift from reactive AML investigations to proactive monitoring and real-time risk detection. This article delves into key takeaways from that session, showcasing how connected data analysis and integration techniques can make AML and KYC processes more efficient. Table of contents: The Power of Data Integration in AML Investigations One of the most powerful aspects of the innovative approach to data analysis using graph technology is its ability to pull data from multiple sourcesand resolve entities across different datasets. In this case, different disconnected data from the OCCRP, Companies House, and open sanctions lists were combined using GraphAware Hume to provide a fuller picture of LCML Alliance’s activities. For example, the same company appeared in both the OCCRP dataset and the UK Companies House records, but with different pieces of information attached. Graph technology excels at linking these disparate data points, creating a unified view of the company’s ownership structure, officers, and transactions. The result was a much clearer understanding of how a company operates within the broader network of financial crime. This ability to integrate data from multiple sources is crucial in modern AML investigations, where criminals often use a combination of fake companies, shell corporations, and cross-border transactions to obscure their activities.By combining data sources and resolving entities, analysts can identify real-world connections that are deliberately obfuscated by proxy or non-existent shareholders and would otherwise go unnoticed. Visualising Financial Networks One of the key challenges in financial crime and AML investigations is understanding the intricate web of transactions, entities, and individuals involved. Traditional approaches often focus on transaction monitoring alone, but this can miss the bigger picture. Connected data analytics platforms like GraphAware Hume, however, offer a powerful alternative by visualising financial networks in a natural way, as a human brain would understand it. In the Azerbaijani Laundromat case, the Hume platform mapped relationships between bank accounts, organisations, and individuals. The data included information from the Organized Crime and Corruption Reporting Project (OCCRP), detailing transactions between entities such as LCML Alliance LLP and numerous others. By placing this data into a graph format, investigators could easily visualise clusters identifying the most influential companies in the network that funnelled money through the same accounts or shared other suspicious connections. The graph representation highlighted how certain entities were intertwined in a broader financial network, revealing links that would have been hard to detect through spreadsheets or isolated transactional reports. Identifying Red Flags Through Data Analysis Connected data analysis not only improves visualisation but also simplifies the identification of red flags—warning signs that a transaction or entity might be involved in illicit activities. In the webinar, four sig",financial crime technology,"AML investigations, network analytics, connected data analysis",The article discusses the use of connected data analysis tools in Anti-Money Laundering (AML) investigations.,AML investigations is the primary focus of the article.; Network analytics is the method used to detect financial crime.; Connected data analysis tools are highlighted as a transformative technology in AML investigations.
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5120765,The Legitimation of Shareholder Primacy by Ann Lipton :: SSRN,"The Legitimation of Shareholder Primacy Journal of Corporation Law (forthcoming) European Corporate Governance Institute - Law Working No. 826/2025 Tulane Public Law Research Paper No. 25-1 44 PagesPosted: 3 Feb 2025Last revised: 19 Feb 2025 Ann Lipton Tulane University - Law School; Tulane University - The Murphy Institute; European Corporate Governance Institute (ECGI) Date Written: February 01, 2025 Abstract We are living in a particularly polarized era, and corporate governance is no exception. With controversies raging over ""environmental, social, governance"" (ESG) investing, diversity, equity, and inclusion initiatives, climate change as an investment concern, and even Elon Musk's pay package at Tesla, it seems as though corporate governance has never been so starkly divided along partisan lines.The divisions have threatened to spill over to Delaware, the preferred jurisdiction for incorporation in the United States. Several high profile cases-including those involving Elon Musk-have called Delaware's neutrality into question. Commenters have argued that Delaware's newly-politicized approach threatens to splinter the corporate governance universe, driving corporations to other states that are more reliable (or that follow different corporations' preferred politics).This Article argues that, in some ways, the critics are correct: Delaware law is on a path toward politicization. But it is not because of any particular bias of its judges or its law; to the contrary, the pressures toward politicization are inherent in any system that purports to guide how vast aggregations of capital will be deployed. What is unique about the current moment is that the trends toward politicization result from tensions inherent in shareholder primacy. Shareholder primacy was conceived, in large part, as a compromise to keep politics out of business management; what the modern controversies reveal is the futility of that effort.  Suggested Citation:Suggested Citation Ann Lipton (Contact Author) Tulane University - Law School  (email) 6329 Freret StreetNew Orleans, LA  70118United States Tulane University - The Murphy Institute  (email) 6823 St Charles AveNew Orleans, LA  70118United States European Corporate Governance Institute (ECGI)  (email) c/o the Royal Academies of BelgiumRue Ducale 1 Hertogsstraat1000 BrusselsBelgium Do you have a job opening that you would like to promote on SSRN? Paper statistics Related eJournals Tulane University School of Law Public Law & Legal Theory Research Paper Series Tulane University School of Law Public Law & Legal Theory Research Paper Series Subscribe to this free journal for more curated articles on this topic Corporate Governance Law eJournal Corporate Governance Law eJournal Subscribe to this fee journal for more curated articles on this topic Corporate & Takeover Law eJournal Corporate & Takeover Law eJournal Subscribe to this fee journal for more curated articles on this topic European Corporate Governance Institute (ECGI) - Law Working Paper Series European Corporate Governance Institute (ECGI) - Law Working Paper Series Subscribe to this free journal for more curated articles on this topic Corporate Governance & Law eJournal Corporate Governance & Law eJournal Subscribe to this fee journal for more curated articles on this topic Corporate Governance: Internal Governance, Organization & Processes eJournal Corporate Governance: Internal Governance, Organization & Processes eJournal Subscribe to this fee journal for more curated articles on this topic Corporate Governance: Economic Consequences, History, Development & Methodology eJournal Corporate Governance: Economic Consequences, History, Development & Methodology eJournal Subscribe to this fee journal for more curated articles on this topic Corporate Governance: Actors & Players eJournal Corporate Governance: Actors & Players eJournal Subscribe to this fee journal for more curated articles on this topic Corporate Governance: Arrangements & Laws eJournal Corporate Governance: Arrangements & Laws eJournal Subscribe to this fee journal for more curated articles on this topic Law & Economics eJournal Law & Economics eJournal Subscribe to this fee journal for more curated articles on this topic Environmental, Social & Governance (ESG) Research Hub eJournal Environmental, Social & Governance (ESG) Research Hub eJournal Subscribe to this free journal for more curated articles on this topic SSRN Quick Links SSRN Rankings About SSRN All content on this site: Copyright © 2024 Elsevier Inc., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the relevant licensing terms apply. We use cookies to help provide and enhance our service and tailor content. To learn more, visitCookie Settings.This page was processed by aws-apollo-l200 in0.063seconds",corporate governance,"shareholder primacy, Delaware law, politicization","The article discusses the governance structure and politics of corporations, specifically focusing on Delaware law.",Shareholder primacy is a central concept in corporate governance.; Delaware law is the preferred jurisdiction for incorporation in the United States.; Politicization refers to the influence of politics on business management.
https://www.powermag.com/software-hardware-innovation-all-needed-to-upgrade-the-power-grid/,"Software, Hardware, Innovation All Needed to Upgrade the Power Grid","POWERis at the forefront of the global power market, providing in-depth news and insight on the end-to-end electricity system and the ongoing energy transition. We strive to be the “go-to” resource for power professionals, offering a wealth of information on innovative business practices, sound safety measures, useful productivity enhancements, and much more. Also In This Issue February 10, 2025 Follow Us POWERis at the forefront of the global power market, providing in-depth news and insight on the end-to-end electricity system and the ongoing energy transition. We strive to be the “go-to” resource for power professionals, offering a wealth of information on innovative business practices, sound safety measures, useful productivity enhancements, and much more. Software, Hardware, Innovation All Needed to Upgrade the Power Grid Enhancing the transmission and distribution of electricity is a priority to ensure a reliable and resilient power supply, as demand increases and grid challenges mount. Providing more electricity to meet growing global demand for power has put a spotlight not only on adding more generation to the grid, primarily through construction of new power plants, but on improvements to the grid itself. Enhancements to support power grid reliability include investments for a variety of technologies, such as battery energy storage systems (BESS), advanced transmission lines, smart grid infrastructure, and distributed generation. Upgrades also focus on improved grid monitoring and control systems that allow for better and faster response to fluctuations in demand and disruptions caused by weather, equipment, or other issues. “Needed enhancements span across condition monitoring, advanced data analytics, fire mitigation, and improved system control mechanisms,” said John Russell, senior director, Solution Consulting atAspenTech, a provider of software and services for the process industries. “Reliability in these systems is crucial not only for preventing downtime but also for ensuring that power is consistently delivered to consumers with minimal disruptions. Specifically, key areas for enhancement include the implementation of condition-based maintenance, improved data collection with smart meters, AI [artificial intelligence]-powered asset management, and advanced control technologies for both transmission and generation systems. By integrating modern technologies, these systems can proactively address failures, optimize performance, and respond to real-time conditions.” “Reliability is central to the modern power grid, especially as we integrate more renewables and electrify transportation and heating. Much work has been done in generation and distribution, driven by smarter hardware, predictive software, and data analytics,” said Brandon Young, CEO atPayless Power, a Texas-based electricity group. “On the generation side, predictive maintenance technologies become game-changers. IoT [Internet of Things] sensors and AI are monitoring equipment like turbines and transformers in real time to pinpoint potential failures before they occur. That way, downtime is kept to a minimum, and power generation stays efficient.” Utilities and grid operators are studying a variety of ways to improve the reliability of power generation and delivery, with data at the heart of much of the research. Want to learn more about innovation that supports reliability and resiliency of the power grid?Read this POWER Interview with professionals from Engineering Design & Testing Corp., a forensic engineering group that works with a range of clients in the power industry. The company offers technical expertise to solve technical and often complex problems associated with power generation and delivery. “Reliability enhancements should be approached as a comprehensive, multi-pronged strategy that addresses design concerns and utilizes new and emerging technologies for improved data analytics,” said Michael Bennett, chief transformation officer at Powin, a battery energy storage company. “Up front, suppliers need to design and engineer components with reliability as a core principle. This includes using advanced materials, robust designs, and high-quality manufacturing processes.” Bennett added, “Ensuring transmission, distribution, and generation assets are equipped with IoT sensors for real-time data collection, monitoring, and reporting capabilities has become a core focus over the past few years as well. These sensors provide operators with actionable insights into system performance and potential issues, allowing them to leverage both cloud and on-premise data to uncover complex insights, correlations, and predictions.” Thomas L. Keefe, vice chair and U.S. Power, Utilities & Renewables Sector leader forDeloitte, said, “Increasing visibility and control through advanced grid technologies” is a major part of increasing grid reliability. “Sensors embedded throughout the network, including smart meters, automated control syst",power and utilities,"power grid, enhancements, reliability",The article discusses the power grid and its improvements to ensure a reliable and resilient power supply.,Power grid is the main subject of the article.; Enhancements refers to the upgrades for the grid mentioned in the text.; Reliability explains the focus on ensuring the stability of the power system.
https://wattclarity.com.au/articles/2025/02/nemde-nightmares-parallel-pathways-and-clashing-constraints/,NEMDE nightmares - parallel pathways and clashing constraints - WattClarity,"NEMDE nightmares – parallel pathways and clashing constraints Posted byAllan O'NeilWednesday 19thFebruary 2025  4:15 PMTopic:2025-02-11,How dispatch works,Market Operations,PEC (Project Energy Connect),Review of price outcomes I was recently involved in a training session for users of Global-Roam’sez2viewsoftware where an attendee asked an innocent question about an odd-looking recent price outcome in South Australia. Finding the answer involved descending a fairly deep rabbit hole, but since it’s a good illustration of: we thought it would be worth writing up for those interested. If you’re not sure what “NEMDE” is, and haven’t grappled in some way withNEM constraints, this probably isn’t the post for you. Otherwise buckle in. Down the rabbit hole This was the situation for the19:05 dispatch interval (DI), NEM time, onTuesday 11th Feb 2025as shown in part of ez2view’s NEM Map widget. The innocent question:where did the $4,950/MWh price in South Australia (thatPaul recorded here earlier) come from? What are interconnector limits again? A closer look at the two interconnectors between South Australia and Victoria also raises questions: To explain what this second question’s even about, here’s a blow-up of the interconnector information shown above:  “V-S-MNSP1” is theMurraylink DC cablerunning from north-west Victoria to mid-north South Australia. “V-SA” is – or looks like – the well-knownHeywood interconnectorin the far south-west / south-east of the two states. The large numbers show thetarget transfer– energy flow in megawatts – across each interconnector, with the icon shape indicating flow direction. The two pairs of smaller numbers with directional arrowheads are (supposedly) the limits on flows when exporting from Victoria to South Australia (on the left) and when importing from South Australia into Victoria (on the right). Most newcomers to the NEM start with the very reasonable assumption that these flow limits will be something to do with how bigthe transmission lines and cables between regionsare, so shouldn’t differ too much if at all from the inherent physical capacity of these links. Unfortunately that’s often not the case. So what does set them? In practice, the limits arevery oftennothing to do with physical capacity. They are generally set by considerations ofsystem security, or what levels of transfer are possible while keeping all aspects of power system operation robust against disturbances that can inevitably buffet the grid – generators or transmission lines tripping, lightning strikes etc. These limits are dynamically calculated based on the actual state of the power system at any time and can move around a lot. In this case, these limits are saying that Murraylink can’timportmore than the 99 MW it’s carrying into Victoria (this limit is indicated at the bottom right of the icon by the right-facing arrowhead and ‘99’) – sounds reasonable. But at bottom left the Murraylink icon shows that its maximumexportto South Australia is -548 MW. That’snegative(indicated by another right-facing arrowhead) 548 MW into South Australia, meaning Murraylink should be flowingat least548 MWeastwards. This isn’t remotely physically possible, and anyway it completely conflicts with the 99 MW ‘maximum import’ limit. Things are not much better for V-SA, which has a target flow of 277 MW west from Victoria into South Australia, exceeding an export limit of 259 MW. The graphic also shows an import limit implying that V-SA shouldn’t carry more than-353 MWinto Victoria, ie that it should be flowingat least 353 MW westward. Again, in total conflict with the nominal export limit. After all that time explaining what the limit question is about, I’m not going to answer it here except to say it indicates that the dispatch process was facing great difficulties coming up with a pattern of generation dispatch and inter-regional flows that satisfied all the requirements being placed on it. We’ll see further down what those difficulties were. Where’s PEC? Back to the simpler question – where is PEC-1? An actualnetwork maphelps here: The ‘V-SA’ interconnector used in the market dispatch process now comprisestwo physical AC linksin parallel, the familiar Heywood 500kV/275kV link running through the southwest of Victoria / southeast South Australia, and the new PEC-1 330kV/220kV connection between Bundey in mid-north South Australia and Buronga in NSW which then connects into northwest Victoria via Red Cliffs. Flows across these paths can’t be independently controlled – they follow the simple physics of AC networks. Market dispatch treats the two as a single interconnection whose flow is the sum of physical flows on the two links. For anyone wondering how two parallel links can be controlled in combination, when neither of them can be individually – good question.They can’t. All the dispatch process can do for AC-connected regions is set targets for controllable generators and loads on either side of their boundary (as well ",federated search,"NEMDE, dispatch interval, interconnector",The article discusses a federated search scenario in the context of energy market operations.,"NEMDE refers to the National Electricity Market Dispatch Engine, which is the system used for coordinating electricity generation and distribution in Australia.; Dispatch interval is a term used in energy market operations to refer to the short periods of time (usually 5 minutes) over which the dispatch engine determines the optimal allocation of power resources.; Interconnector refers to the infrastructure that connects different regions or grids in an electricity network, allowing for the transfer of electricity between them."
https://github.com/kyribaker/7bus_LMPs,GitHub - kyribaker/7bus_LMPs: An illustration of how congestion causes high locational marginal prices in a 7-bus power grid.,"Navigation Menu Search code, repositories, users, issues, pull requests... Provide feedback We read every piece of feedback, and take your input very seriously. Saved searches Use saved searches to filter your results more quickly To see all available qualifiers, see ourdocumentation. An illustration of how congestion causes high locational marginal prices in a 7-bus power grid. Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. kyribaker/7bus_LMPs Folders and files Latest commit History Repository files navigation A jupyter notebook illustrating the following example of a DC optimal power flow (OPF) problem solved on a simple 7-bus system. Two congested lines create a locational marginal price (LMP) that exceeds ($90/MWh) the cost of the highest priced generator in the grid ($45/MWh).  About An illustration of how congestion causes high locational marginal prices in a 7-bus power grid. Resources Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. Stars Watchers Forks Releases Packages0 Uh oh! There was an error while loading.Please reload this page. There was an error while loading.Please reload this page. Languages Footer Footer navigation",power and utilities,"locational marginal prices, DC optimal power flow, 7-bus system","The article discusses the management of a power grid, specifically focusing on locational marginal prices within a 7-bus system.",Locational marginal prices are the main focus of the article.; DC optimal power flow is the method used to solve the problem.; The 7-bus system is the specific example given in the text.
https://www.abc.net.au/news/2024-10-13/australian-coal-plant-in-extraordinary-survival-experiment/104461504,"Australian coal plant in 'extraordinary' survival experiment as solar, funding woes stalk industry - ABC News"," Search the news, stories & people Your ABC Account Personalise the news and stay in the know Emergency Backstory Newsletters 中文新闻 BERITA BAHASA INDONESIA TOK PISIN Australian coal plant in 'extraordinary' survival experiment as solar, funding woes stalk industry Topic:Coal Coal plants like Bayswater have been the pillars of Australia's power supply. But that's changing.(Reuters: Loren Elliott) After years of setbacks, bad news and mounting obstacles, Australia's coal-fired generators must have felt they had something to celebrate. AGL, the giant energy company backed by tech billionaire and climate evangelist Mike Cannon-Brookes, revealed it had pulled off a first. At its huge Bayswater power station in the Hunter Valley north of Sydney, AGL successfully switched off an entire unit before switching it back on again just five hours later – a feat until recently considered unthinkable. In a post on social media, Bayswater general manager Len McLachlan said this process of ""two-shifting"" was a harbinger of the future. Throughout its history, coal-fired power had been considered the quintessential provider of so-called base-load electricity. The term refers to generation sources that run round-the-clock, throughout the year and more or less at their full capacity. It was a function that coal plants had carried out for decades, and a business model on which the industry was built. The rise and rise of rooftop solar in Australia has been breathtaking.(ABC News: Glyn Jones) But, more and more, base-load coal plants have been squeezed out of the market as ever more renewable energy – particularly solar power – flooded the system. In doing so, renewable energy was forcing wholesale prices ever lower – so low that they were entering negative territory where generators had to pay to keep producing. And while coal plants – despite the orthodox thinking – could in fact reduce their output to minimise their exposure to those low or negative prices, they were only able to lower production so much. Beyond that, the industry had claimed, it was technically unsafe and economically unsound to operate coal plants designed to run full steam. That was, until now. Times, and coal, are a-changin' Two-shifting, Mr McLachlan said, was a way for coal plants to capitalise on high prices in the evening peak while avoiding bearish prices in the middle of the day when solar power was most abundant. It was a way to give flexibility to a type of generation that was not designed to be flexible. Mike Cannon-Brookes.(AAP: Dan Himbrechts) ""Bayswater power station achieved a major milestone recently with the successful completion of our first two-shift trial,"" Mr McLachlan wrote. ""Our team desynchronised 20 seconds ahead of the 10am target and re-synchronised within 50 seconds of the 3pm target. David Leitch, an energy industry analyst, said it was a significant achievement by AGL. ""The general presumption has been that coal-fired generation units, which are very large units … have a minimum operating rate below which they can't run,"" Mr Leitch said. ""The presumption was that when you turned them off you then had to let them cool down and start them up fairly gradually. ""You couldn't stop and start them in the same way that you could with a gas generator, for instance, let alone a battery that can change direction in milliseconds. ""But the exciting news out of Bayswater is that it can now be turned off and on within a 24-hour cycle on a continuous basis without damaging anything very much. Mr Leitch said that on top of the financial savings for their owners, switching off coal plants in the middle of the day would also cut emissions and make more room for renewable energy. The floor below which coal output could not drop is seen by industry players as an impediment to Australia's energy transition – a cause to waste huge amounts of cheap wind and solar output at certain times. These times are most obvious in the middle of the day when solar output is highest, particularly in autumn and spring, when relatively mild weather means demand for power can be subdued. ""It's only still only experiments at this stage,"" he said. Now comes the bad news But mere days after AGL's announcement of the good news, another Australian coal-fired generator painted a bleaker, arguably truer, picture for the industry. Delta, the Czech firm that owns the aging Vales Point coal plant near Lake Macquarie, reported that it had been unable to get the financial backing of any major bank in Australia. And Delta, like any other generator in the national electricity market covering Australia's eastern seaboard, needs financial backing from the banks. Vales Point Power Station(ABC News: Ben Millington) Each day, the generators are producing and selling vast quantities of electricity, trading with customers more or less continuously. To cover periods where a company like Delta might sell power at a loss and incur any debts, the Australian Energy Market Operator requires genera",power and utilities,"coal-fired power, renewable energy, base-load electricity",The article discusses the challenges faced by coal-fired power plants due to the rise of renewable energy.,"Coal-fired power is the main focus of the article.; Renewable energy, particularly solar power, is discussed as a challenge for coal plants.; Base-load electricity is a key concept related to coal-fired power."
https://blog.gridstatus.io/curtailment/,Curtailment: When We Throw Away Clean Energy,"Curtailment: When We Throw Away Clean Energy As we approach the end of another shoulder season — a period of lower demand and higher transmission outages — it's a good time to explain curtailment, a crucial yet often controversial aspect of today's grid operations. Grid Status As renewable energy from solar and wind expands across the country, surpluses in generation will become increasingly common and consequently lead to curtailment, which is the deliberate reduction of output below what could be produced. As a concept and in practice, curtailment is interwoven into conversations, questions, and decisions about the modern grid. On its face, curtailment is a waste of “free” electricity from clean sources, but why does it occur? Is the answer as simple as building more transmission, as often presented? Electricity markets are a construct of human administrative engineering, so recognizing their limitations and how those constraints can lead to unintuitive outcomes is important to understanding curtailment. In this post, we analyze the dynamics behind renewable curtailment under three different (albeit interrelated) market circumstances in the areas that have the highest penetrations of renewables in the United States: California Independent System Operator (CAISO), Electric Reliability Council of Texas  (ERCOT), and Southwest Power Pool (SPP). Table of Contents A primer on curtailment In nodal electricity markets, curtailment refers to the intentional reduction in the production of electricity by certain generation resources. When used in conversation today, it almost always refers to zero cost renewable resources, namely wind and solar. While curtailment is often seen as undesirable, especially from the perspective of renewable energy producers, is an essential mechanism for market operators to maintain a reliable and stable electricity grid. Curtailment and LMPs Curtailment typically results from a negative price signal to generators via theLocational Marginal Pricing, or LMP. The LMP reflects the value of electricity at a specific location, taking into account the costs of generation, transmission constraints, and transmission losses. When the LMP is sufficiently negative, it becomes uneconomical for a unit to generate its full output, leading to curtailment. The LMP can be viewed as a representation of the administrative engineering needed for the operation of wholesale electricity markets. In the following sections, we examine different circumstances that all lead to the same outcome - a strongly negative congestion component of the LMP. Curtailment may also occur via specific, targeted, operator actions (such as theSeptember 6th event in ERCOT), but here we will largely focus on market fundamentals that influence LMP formation. Low demand for excess California Solar Let's start by exploring a fundamental driver of curtailment – a lack of demand for the renewable capacity that has been built. California has seen explosive growth in solar generation, with solar resources built to serve the summer peak leading to excess capacity during periods of low demand. The scatter plot below lays bare the existence of a relationship between curtailment and low load. In a way, this graph shows a properly functioning market, as higher prices tend to be correlated with lower levels of curtailment, while very low prices are properly signaling the existence of excess generation exacerbating congestion. The solar buildout is so large that CAISO is often exporting during peak sunny hours, evenwhile running gas plants internally. But, what happens when the regional grid can't use all of California's solar? While it still relies on interchange with neighbors, the timing and depth of the relationship has shifted due to renewable generation as we can see below CAISO has undergone quite a transformation over the past decade. As solar has expanded its role in daily CAISO generation, exports have become more common, but when regional demand is satiated, curtailment becomes the only option. With economic incentives to deliver as much renewable energy as possible, conditions can occur in which a given area has energy production above its load. Here, we see the relationship betweeninterchange,solar generation, andcurtailmentin CAISO. A few things stick out It is tempting to conclude that any additional load while there is curtailment would have zero marginal emissions because it would not actually require more fossil fuel generation. However, in the next two sections, we will explain how not all curtailment is caused by insufficient load. Transmission Congestion in Texas Utility-scale renewables often depend on transmission lines to allow electricity to flow from where it is generated to where it is needed. When transmission bottlenecks arise, the system operator may have to curtail generation from certain sources to ensure grid stability. No place is a better example of this dynamic than Texas, where disparities between the geogra",power and utilities,"curtailment, renewable energy, electricity markets","The article discusses the concept, causes, and implications of curtailment in power grid operations, particularly focusing on renewable energy sources.",Curtailment is a central topic of the article.; Renewable energy (wind and solar) is the main focus of the discussion.; Electricity markets are analyzed in relation to curtailment.
https://blog.gridstatus.io/spp-expansion-west/,SPP Expansion Provides a Blueprint for the Future of the Grid,"SPP Expansion Provides a Blueprint for the Future of the Grid The U.S. power grid is so complex that trivial changes are hard to comprehend and harder to implement. Yet, a recent announcement by the Southwest Power Pool (SPP) is closer to historic than trivial, making it a good case study for a more integrated and efficient grid. Grid Status SPP, the organized electricity market dominating the Great Plains,announced a substantial expansionof their footprint to the West earlier this month. When it goes into effect in early 2026, it will be the largest RTO/ISO expansion since MISO South in 2013. What made this announcement particularly notable is that the joining entities mostly operate in the Western Interconnection. This expansion makes SPP the first RTO in the US to bridge that gap. Given the importance of this change we wanted to explore the interchange, fuel mix, and emissions data for relevant Balancing Authorities (BAs) using the newEIA Data Browseron Grid Status. As we will show, greater connectivity and unified planning over the patchwork of the West can help to facilitate a reduction in both power prices as well as total emissions. Grids in Transition Two new Balancing Authorities are joining SPP,WACMandWAUW.WACM is managed by the Rocky Mountain region of the Western Area Power Administration (WAPA) while WAUW is under the control of the Upper Great Plains region of WAPA. WAPA itself was originally created to market and transmit electricity from federal hydropower facilities, but by developing into BAs these regions became more entangled in the wider resource mix of the west. Fuel mix is available for every BA in theEIA-930 data, whether interesting or not. In the case of WAUW it’s only interesting in a historic context - it looks like an isolated grid from the 1900s,totalling under 100 MW of pure hydropower- directly in line with the original WAPA mission. WACM vs SPP Fuel Mix WACM, however, is more interesting, managing a larger area with the resources of multiple entities, and is a great representative example for the continued evolution of the grid. If you squint a bit, the contribution of different resources to WACM does not look altogether dissimilar to where SPP itself was only 12 years ago (albeit with a greater amount of hydropower in place of natural gas). Just over the last year you can see the natural gas generation begin to grow and coal begins to drop off somewhat. Hydropower helps in displacing some coal - the West had an excellent winter from a water-recharge perspective - but you still see natural gas running more frequently as demand has also picked up. For comparison, here’s SPP’s fuel mix over the same time frame and also from the EIA data (which does have some inconsistencies with the ISO-reported values). SPP has a lot more wind, but also far more natural gas. SPP represents something like what we may see WACM transition towards - an increase in wind and solar, along with some natural gas, accompanied by a reduction in coal’s share of generation. WACM vs SPP Emissions The preponderance of coal in WACM is also borne out by the BA’s CO2 emissions factor from the EIA. In comparison to SPP, generation in WACM’s footprint not only emits substantially more CO2 per MWh of production, but is in a smaller range due to the relatively low penetration of variable renewables such as wind and solar. WACM vs SPP Pricing Additionally, there is increased potential for trading between the markets, both via the limited interconnection capacity of today as well the transmission lines of tomorrow. You can already look at real time prices today in both BAs to see the impact wind-heavy western SPP could have on coal-heavy WACM. Indeed, this process is already ongoing - in 2022, 400 MW of coal in WACM retired. SCSE’s mean daily price is often lower than the WACM node, particularly during the time of year in which hydropower resources are in the part of their annual cycle waiting for “recharge” from snowpack and the “missing” hydropower generation is replaced by technologies that have fuel costs.  This bodes well for the transition of WACM, and in these graphs you can see a blueprint for the transition of coal heavy regions. Bridging the Gap When this change goes into effect, SPP will be the first RTO to breach a historic barrier in the US: coordinated operation across two grids. It may surprise some readers to discover that the US consists of not one, but three “grids”. Other than Eastern and Western Interconnections, the third isERCOT’s, which encompasses most, but not all, of Texas. There are historic and political reasons for the three grids, which we won’t dive into here (although check out thisfun Planet Money episodefor a taste of the ERCOT grid). PerNREL’s Seams study, there is only ~1,300 MW of direct connection between the eastern and western grids. To describe the technical obstacle very simply, while the two grids operate at the same 60Hz frequency (unlikeJapan), their actual AC currents ar",power and utilities,"US power grid, RTO/ISO expansion",The article discusses the expansion of an organized electricity market (RTO/ISO) in the US power grid.,US power grid is the central topic of the article.; RTO/ISO expansion is the significant event discussed.
https://www.eia.gov/outlooks/steo/,Short-Term Energy Outlook - U.S. Energy Information Administration (EIA),"U.S. Energy Information Administration - EIA - Independent Statistics and Analysis Menu Petroleum & Other Liquids Crude oil, gasoline, heating oil, diesel, propane, and other liquids including biofuels and natural gas liquids. Natural Gas Exploration and reserves, storage, imports and exports, production, prices, sales. Electricity Sales, revenue and prices, power plants, fuel use, stocks, generation, trade, demand & emissions. Consumption & Efficiency Energy use in homes, commercial buildings, manufacturing, and transportation. Coal Reserves, production, prices, employment and productivity, distribution, stocks, imports and exports. Renewable & Alternative Fuels Includes hydropower, solar, wind, geothermal, biomass and ethanol. Nuclear & Uranium Uranium fuel, nuclear reactors, generation, spent fuel. Total Energy Comprehensive data summaries, comparisons, analysis, and projections integrated across all energy sources. A-Z Index Analysis & Projections Monthly and yearly energy forecasts, analysis of energy topics, financial analysis, congressional reports. Markets & Finance Financial market analysis and financial data for major energy companies. Environment Greenhouse gas data, voluntary reporting, electric power plant emissions. Energy Disruptions Maps, tools, and resources related to energy disruptions and infrastructure. A-Z Index U.S. States State energy information, including overviews, rankings, data, and analyses. Maps Maps by energy source and topic, includes forecast maps. International International energy information, including overviews, rankings, data, and analyses. Regional Dashboards & Data Regional energy information including dashboards, maps, data, and analyses. A-Z Index Glossary Data Tools, Apps, & Maps Tools to customize searches, view specific data sets, study detailed documentation, and access time-series data. Open Data EIA's free and open data available as API, Excel add-in, bulk files, and widgets EIA Beta Come test out some of the products still in development and let us know what you think! Open Source Code EIA's open source code, available on GitHub. All Reports & Publications EIA Survey Forms Forms EIA uses to collect energy data including descriptions, links to survey instructions, and additional information. Email Updates Sign up for email subscriptions to receive messages about specific EIA products RSS Feeds Subscribe to feeds for updates on EIA products including Today in Energy and What's New. Follow us. . .  A-Z Index Today in Energy Short, timely articles with graphics on energy, facts, issues, and trends. Energy Explained For Teachers Lesson plans, science fair experiments, field trips, teacher guide, and career corner. Glossary FAQs A-Z Index What's New? Press Room Coming Up Coming Up Featured Reports Reports requested by congress or otherwise deemed important. A-Z Index Short-Term Energy Outlook Release Date:May 6, 2025			 |Forecast Completed:May 1, 2025			 |Next Release Date:June 10, 2025|Full Report|Text Only|All Tables|All Figures Forecast overview Note: Values in this table are rounded and may not match  values in other tables in this report. Percentages are calculated from unrounded values.The current STEO forecast was released May 6.The previous STEO forecast was released April 10. You can find more information in thedetailed table of forecast changes. Interactive Data Viewers Provides custom data views of historical and forecast dataSTEO Data browser  ›Real Prices Viewer › About theShort-Term Energy Outlook Previous STEO Forecasts: Other EIA Forecasts:",power and utilities,"crude oil, natural gas, electricity","The article provides independent statistics and analysis on various aspects of energy, primarily power and utilities.","Crude oil is one of the main topics covered in the article.; Natural gas is another significant focus of the article.; Electricity is discussed extensively, including generation, sales, and consumption."
